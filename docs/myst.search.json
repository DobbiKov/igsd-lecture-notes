{"version":"1","records":[{"hierarchy":{"lvl1":"IGSD Lectures"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"IGSD Lectures"},"content":"This is a place where I write my lectures for the info-graphics course.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Understanding atan2 function"},"type":"lvl1","url":"/atan2","position":0},{"hierarchy":{"lvl1":"Understanding atan2 function"},"content":"","type":"content","url":"/atan2","position":1},{"hierarchy":{"lvl1":"Understanding atan2 function","lvl2":"‚ö†Ô∏è Why Do Signs Differ in atan2?"},"type":"lvl2","url":"/atan2#id-why-do-signs-differ-in-atan2","position":2},{"hierarchy":{"lvl1":"Understanding atan2 function","lvl2":"‚ö†Ô∏è Why Do Signs Differ in atan2?"},"content":"Because in 2D space, the same slope (dy/dx) can belong to two completely different directions ‚Äî depending on the signs of x and y, which determine the quadrant.\n\nLet‚Äôs break this down:","type":"content","url":"/atan2#id-why-do-signs-differ-in-atan2","position":3},{"hierarchy":{"lvl1":"Understanding atan2 function","lvl3":"üéØ Tangent Only Knows the Slope, Not the Direction","lvl2":"‚ö†Ô∏è Why Do Signs Differ in atan2?"},"type":"lvl3","url":"/atan2#id-tangent-only-knows-the-slope-not-the-direction","position":4},{"hierarchy":{"lvl1":"Understanding atan2 function","lvl3":"üéØ Tangent Only Knows the Slope, Not the Direction","lvl2":"‚ö†Ô∏è Why Do Signs Differ in atan2?"},"content":"The regular inverse tangent (atan(y/x)) only gives you the angle from the positive x-axis and doesn‚Äôt know what quadrant the vector is in.\n\nFor example:atan(3/2) ‚âà 56.3¬∞\natan(-3/-2) = atan(3/2) ‚âà 56.3¬∞\n\nSo both (3,2) and (-3,-2) give the same result ‚Äî but they‚Äôre in opposite quadrants!","type":"content","url":"/atan2#id-tangent-only-knows-the-slope-not-the-direction","position":5},{"hierarchy":{"lvl1":"Understanding atan2 function","lvl3":"üß≠ Enter atan2(dy, dx)","lvl2":"‚ö†Ô∏è Why Do Signs Differ in atan2?"},"type":"lvl3","url":"/atan2#id-enter-atan2-dy-dx","position":6},{"hierarchy":{"lvl1":"Understanding atan2 function","lvl3":"üß≠ Enter atan2(dy, dx)","lvl2":"‚ö†Ô∏è Why Do Signs Differ in atan2?"},"content":"atan2(y, x) uses the signs of both x and y to return the angle in the correct quadrant.\n\nHere‚Äôs how it resolves direction:\n\nQuadrant\n\nx sign\n\ny sign\n\natan2(y, x) result\n\nI\n\n+\n\n+\n\n0 to +œÄ/2\n\nII\n\n-\n\n+\n\n+œÄ/2 to +œÄ\n\nIII\n\n-\n\n-\n\n-œÄ to -œÄ/2\n\nIV\n\n+\n\n-\n\n-œÄ/2 to 0\n\nThis means:\n\n(3,2) is in Quadrant I ‚Üí angle ‚âà +56.3¬∞\n\n(-3,-2) is in Quadrant III ‚Üí angle ‚âà -123.7¬∞\n\nBoth have the same dy/dx = 2/3, but the direction is flipped ‚Äî and so is the sign of the angle.","type":"content","url":"/atan2#id-enter-atan2-dy-dx","position":7},{"hierarchy":{"lvl1":"Understanding atan2 function","lvl3":"üß† Intuition with Geometry","lvl2":"‚ö†Ô∏è Why Do Signs Differ in atan2?"},"type":"lvl3","url":"/atan2#id-intuition-with-geometry","position":8},{"hierarchy":{"lvl1":"Understanding atan2 function","lvl3":"üß† Intuition with Geometry","lvl2":"‚ö†Ô∏è Why Do Signs Differ in atan2?"},"content":"Imagine a unit circle. You draw a line from the origin to point (3, 2) and another to (-3, -2).\n\nThey both form a triangle with same shape (i.e., same slope),\n\nBut they‚Äôre on opposite sides of the origin,\n\nSo the angle from the positive x-axis must reflect that ‚Üí one is positive, the other negative (or over œÄ radians).","type":"content","url":"/atan2#id-intuition-with-geometry","position":9},{"hierarchy":{"lvl1":"Understanding atan2 function","lvl3":"‚úÖ Why This Matters","lvl2":"‚ö†Ô∏è Why Do Signs Differ in atan2?"},"type":"lvl3","url":"/atan2#id-why-this-matters","position":10},{"hierarchy":{"lvl1":"Understanding atan2 function","lvl3":"‚úÖ Why This Matters","lvl2":"‚ö†Ô∏è Why Do Signs Differ in atan2?"},"content":"If you only used atan(dy/dx), you‚Äôd get the same result for both vectors ‚Äî and your object might rotate in the wrong direction, e.g., rotating left instead of right.\n\natan2 fixes this by:\n\nDisambiguating the quadrant,\n\nPreserving sign,\n\nProviding smooth, consistent angular rotation across the full circle.\n\nWould you like a quadrant-by-quadrant demo showing how atan2 behaves as a vector moves around the origin?","type":"content","url":"/atan2#id-why-this-matters","position":11},{"hierarchy":{"lvl1":"Cohen Sutherland method"},"type":"lvl1","url":"/cohen-sutherland","position":0},{"hierarchy":{"lvl1":"Cohen Sutherland method"},"content":"typedef int OutCode;\n\nconst int INSIDE = 0b0000;\nconst int LEFT   = 0b0001;\nconst int RIGHT  = 0b0010;\nconst int BOTTOM = 0b0100;\nconst int TOP    = 0b1000;\n\n// Compute the bit code for a point (x, y) using the clip rectangle\n// bounded diagonally by (xmin, ymin), and (xmax, ymax)\n\n// ASSUME THAT xmax, xmin, ymax and ymin are global constants.\n\nOutCode ComputeOutCode(double x, double y)\n{\n\tOutCode code = INSIDE;  // initialised as being inside of clip window\n\n\tif (x < xmin)           // to the left of clip window\n\t\tcode |= LEFT;\n\telse if (x > xmax)      // to the right of clip window\n\t\tcode |= RIGHT;\n\tif (y < ymin)           // below the clip window\n\t\tcode |= BOTTOM;\n\telse if (y > ymax)      // above the clip window\n\t\tcode |= TOP;\n\n\treturn code;\n}\n\n// Cohen‚ÄìSutherland clipping algorithm clips a line from\n// P0 = (x0, y0) to P1 = (x1, y1) against a rectangle with \n// diagonal from (xmin, ymin) to (xmax, ymax).\nbool CohenSutherlandLineClip(double& x0, double& y0, double& x1, double& y1)\n{\n\t// compute outcodes for P0, P1, and whatever point lies outside the clip rectangle\n\tOutCode outcode0 = ComputeOutCode(x0, y0);\n\tOutCode outcode1 = ComputeOutCode(x1, y1);\n\tbool accept = false;\n\n\twhile (true) {\n\t\tif (!(outcode0 | outcode1)) {\n\t\t\t// bitwise OR is 0: both points inside window; trivially accept and exit loop\n\t\t\taccept = true;\n\t\t\tbreak;\n\t\t} else if (outcode0 & outcode1) {\n\t\t\t// bitwise AND is not 0: both points share an outside zone (LEFT, RIGHT, TOP,\n\t\t\t// or BOTTOM), so both must be outside window; exit loop (accept is false)\n\t\t\tbreak;\n\t\t} else {\n\t\t\t// failed both tests, so calculate the line segment to clip\n\t\t\t// from an outside point to an intersection with clip edge\n\t\t\tdouble x, y;\n\n\t\t\t// At least one endpoint is outside the clip rectangle; pick it.\n\t\t\tOutCode outcodeOut = outcode1 > outcode0 ? outcode1 : outcode0;\n\n\t\t\t// Now find the intersection point;\n\t\t\t// use formulas:\n\t\t\t//   slope = (y1 - y0) / (x1 - x0)\n\t\t\t//   x = x0 + (1 / slope) * (ym - y0), where ym is ymin or ymax\n\t\t\t//   y = y0 + slope * (xm - x0), where xm is xmin or xmax\n\t\t\t// No need to worry about divide-by-zero because, in each case, the\n\t\t\t// outcode bit being tested guarantees the denominator is non-zero\n\t\t\tif (outcodeOut & TOP) {           // point is above the clip window\n\t\t\t\tx = x0 + (x1 - x0) * (ymax - y0) / (y1 - y0);\n\t\t\t\ty = ymax;\n\t\t\t} else if (outcodeOut & BOTTOM) { // point is below the clip window\n\t\t\t\tx = x0 + (x1 - x0) * (ymin - y0) / (y1 - y0);\n\t\t\t\ty = ymin;\n\t\t\t} else if (outcodeOut & RIGHT) {  // point is to the right of clip window\n\t\t\t\ty = y0 + (y1 - y0) * (xmax - x0) / (x1 - x0);\n\t\t\t\tx = xmax;\n\t\t\t} else if (outcodeOut & LEFT) {   // point is to the left of clip window\n\t\t\t\ty = y0 + (y1 - y0) * (xmin - x0) / (x1 - x0);\n\t\t\t\tx = xmin;\n\t\t\t}\n\n\t\t\t// Now we move outside point to intersection point to clip\n\t\t\t// and get ready for next pass.\n\t\t\tif (outcodeOut == outcode0) {\n\t\t\t\tx0 = x;\n\t\t\t\ty0 = y;\n\t\t\t\toutcode0 = ComputeOutCode(x0, y0);\n\t\t\t} else {\n\t\t\t\tx1 = x;\n\t\t\t\ty1 = y;\n\t\t\t\toutcode1 = ComputeOutCode(x1, y1);\n\t\t\t}\n\t\t}\n\t}\n\treturn accept;\n}","type":"content","url":"/cohen-sutherland","position":1},{"hierarchy":{"lvl1":"Analyzing Cohen-Sutherland Algorithm"},"type":"lvl1","url":"/cohen-sutherland-clipping","position":0},{"hierarchy":{"lvl1":"Analyzing Cohen-Sutherland Algorithm"},"content":"Voici une r√©solution d√©taill√©e, √©tape par √©tape, de l‚Äôalgorithme de Cohen‚ÄìSutherland pour les cinq segments indiqu√©s, dans le rectangle de coins diagonaux (4,4) et  (10,8).","type":"content","url":"/cohen-sutherland-clipping","position":1},{"hierarchy":{"lvl1":"Analyzing Cohen-Sutherland Algorithm","lvl2":"1. Param√®tres et codage des r√©gions"},"type":"lvl2","url":"/cohen-sutherland-clipping#id-1-param-tres-et-codage-des-r-gions","position":2},{"hierarchy":{"lvl1":"Analyzing Cohen-Sutherland Algorithm","lvl2":"1. Param√®tres et codage des r√©gions"},"content":"Rectangle de clippingx_{\\min}=4,\\quad x_{\\max}=10,\\quad y_{\\min}=4,\\quad y_{\\max}=8.\n\nCode √† 4 bits \\bigl[b_\\text{haut},\\,b_\\text{bas},\\,b_\\text{droite},\\,b_\\text{gauche}\\bigr] pour un point P(x,y) :\n\nb_\\text{haut}=1 si y>y_{\\max}, sinon 0\n\nb_\\text{bas}=1 si y<y_{\\min}, sinon 0\n\nb_\\text{droite}=1 si x>x_{\\max}, sinon 0\n\nb_\\text{gauche}=1 si x<x_{\\min}, sinon 0\n\nPoint\n\nCoordonn√©es\n\nCode [H,B,D,G]\n\nA\n\n(2,10)\n\n[1,0,0,1] = 1001\n\nB\n\n(11,9)\n\n[1,0,1,0] = 1010\n\nC\n\n(5,6)\n\n[0,0,0,0]\n\nD\n\n(7,7)\n\n[0,0,0,0]\n\nE\n\n(3,2)\n\n[0,1,0,1] = 0101\n\nF\n\n(11,7)\n\n[0,0,1,0] = 0010\n\nG\n\n(9,5)\n\n[0,0,0,0]\n\nH\n\n(12,5)\n\n[0,0,1,0] = 0010\n\nI\n\n(2,7)\n\n[0,0,0,1] = 0001\n\nJ\n\n(6,9)\n\n[1,0,0,0] = 1000","type":"content","url":"/cohen-sutherland-clipping#id-1-param-tres-et-codage-des-r-gions","position":3},{"hierarchy":{"lvl1":"Analyzing Cohen-Sutherland Algorithm","lvl2":"2. Traitement segment par segment"},"type":"lvl2","url":"/cohen-sutherland-clipping#id-2-traitement-segment-par-segment","position":4},{"hierarchy":{"lvl1":"Analyzing Cohen-Sutherland Algorithm","lvl2":"2. Traitement segment par segment"},"content":"Pour chaque segment [P_1P_2] on calcule :\n\nAND des codes :\n\nsi ‚â† 0 ‚Üí trivial reject (tout √† l‚Äôext√©rieur, m√™me zone)\n\nsi = 0 mais tous les deux codes = 0 ‚Üí trivial accept (tout √† l‚Äôint√©rieur)\n\nsinon ‚Üí il faut couper (clip) successivement les extr√©mit√©s hors-rectangle.","type":"content","url":"/cohen-sutherland-clipping#id-2-traitement-segment-par-segment","position":5},{"hierarchy":{"lvl1":"Analyzing Cohen-Sutherland Algorithm","lvl3":"2.1 Segment [AB]","lvl2":"2. Traitement segment par segment"},"type":"lvl3","url":"/cohen-sutherland-clipping#id-2-1-segment-ab","position":6},{"hierarchy":{"lvl1":"Analyzing Cohen-Sutherland Algorithm","lvl3":"2.1 Segment [AB]","lvl2":"2. Traitement segment par segment"},"content":"Codes : A = 1001, B = 1010 ‚Üí AND = 1000 ‚â† 0 (bit ‚Äúhaut‚Äù commun)\n\nConclusion : le segment est enti√®rement au-dessus de y=8.\n\nAction : rejet trivial, aucune portion n‚Äôest dans le rectangle.","type":"content","url":"/cohen-sutherland-clipping#id-2-1-segment-ab","position":7},{"hierarchy":{"lvl1":"Analyzing Cohen-Sutherland Algorithm","lvl3":"2.2 Segment [CD]","lvl2":"2. Traitement segment par segment"},"type":"lvl3","url":"/cohen-sutherland-clipping#id-2-2-segment-cd","position":8},{"hierarchy":{"lvl1":"Analyzing Cohen-Sutherland Algorithm","lvl3":"2.2 Segment [CD]","lvl2":"2. Traitement segment par segment"},"content":"Codes : C = 0000, D = 0000 ‚Üí AND = 0000 et tous deux nuls\n\nConclusion : le segment est enti√®rement √† l‚Äôint√©rieur.\n\nClipping : pas de d√©coupage, on garde \\bigl[(5,6),(7,7)\\bigr].","type":"content","url":"/cohen-sutherland-clipping#id-2-2-segment-cd","position":9},{"hierarchy":{"lvl1":"Analyzing Cohen-Sutherland Algorithm","lvl3":"2.3 Segment [EF]","lvl2":"2. Traitement segment par segment"},"type":"lvl3","url":"/cohen-sutherland-clipping#id-2-3-segment-ef","position":10},{"hierarchy":{"lvl1":"Analyzing Cohen-Sutherland Algorithm","lvl3":"2.3 Segment [EF]","lvl2":"2. Traitement segment par segment"},"content":"Codes : E = 0101 (bas+gauche), F = 0010 (droite) ‚Üí AND = 0000 ‚Üí d√©coupage n√©cessaire.\n\n√âtape 1 : traiter E (code bas+gauche)\n\nOn examine d‚Äôabord le bit ‚Äúbas‚Äù ‚Üí intersection avec y=4.\n\nParam√®tre t sur la droite \\overline{EF} :y = 2 + 5t = 4 \\;\\Longrightarrow\\; t = \\frac{2}{5} = 0{,}4.\n\nCoordonn√©e correspondante :x = 3 + 8\\cdot0{,}4 = 6{,}2.\n\nPoint d‚Äôentr√©e dans le rectangle :P_1 = (6{,}2,\\;4).\n\n√âtape 2 : traiter F (code droite) sur le segment P_1F\n\nIntersection avec x=10.\n\nSoit P_1=(6{,}2,4), F=(11,7), vecteur (\\Delta x,\\Delta y)=(4{,}8,3).\n\nOn cherche t' tel que 6{,}2 + 4{,}8\\,t' = 10 \\Rightarrow t' = \\tfrac{3{,}8}{4{,}8} \\approx 0{,}7917.\n\nCoordonn√©e y :y = 4 + 3\\cdot0{,}7917 \\approx 6{,}375.\n\nPoint de sortie :P_2 \\approx (10,\\;6{,}375).\n\nR√©sultat pour [EF] :\\bigl[(6{,}2,\\,4),\\,(\\,10,\\,6{,}375)\\bigr].","type":"content","url":"/cohen-sutherland-clipping#id-2-3-segment-ef","position":11},{"hierarchy":{"lvl1":"Analyzing Cohen-Sutherland Algorithm","lvl3":"2.4 Segment [GH]","lvl2":"2. Traitement segment par segment"},"type":"lvl3","url":"/cohen-sutherland-clipping#id-2-4-segment-gh","position":12},{"hierarchy":{"lvl1":"Analyzing Cohen-Sutherland Algorithm","lvl3":"2.4 Segment [GH]","lvl2":"2. Traitement segment par segment"},"content":"Codes : G = 0000, H = 0010 ‚Üí AND = 0000, d√©coupage n√©cessaire (H hors √† droite).\n\nClip de H sur x=10 sur la ligne de hauteur constante y=5 :G=(9,5)\\,\\to\\,H=(12,5)\\quad\\Longrightarrow\\quad P=(10,5).\n\nR√©sultat pour [GH] :\\bigl[(9,5),\\,(10,5)\\bigr].","type":"content","url":"/cohen-sutherland-clipping#id-2-4-segment-gh","position":13},{"hierarchy":{"lvl1":"Analyzing Cohen-Sutherland Algorithm","lvl3":"2.5 Segment [IJ]","lvl2":"2. Traitement segment par segment"},"type":"lvl3","url":"/cohen-sutherland-clipping#id-2-5-segment-ij","position":14},{"hierarchy":{"lvl1":"Analyzing Cohen-Sutherland Algorithm","lvl3":"2.5 Segment [IJ]","lvl2":"2. Traitement segment par segment"},"content":"Codes : I = 0001 (gauche), J = 1000 (haut) ‚Üí AND = 0000, d√©coupage en deux passes.\n\n1. Clip de I (gauche) sur x=4 sur \\overline{IJ} :I=(2,7),\\;J=(6,9)\\;\\Longrightarrow\\;\n  t = \\frac{4-2}{6-2} = \\tfrac{2}{4} = 0{,}5,\\quad\n  P_1 = (4,\\;7 + 2\\cdot0{,}5) = (4,8).\n\n2. Clip de J (haut) sur y=8 entre P_1=(4,8) et J=(6,9) :y = 8 + (9-8)\\,t = 8 \\;\\Rightarrow\\; t=0 \n  \\;\\Longrightarrow\\; \\text{m√™me point }(4,8).\n\nOn obtient un segment d√©g√©n√©r√© :P_1 = P_2 = (4,\\,8).","type":"content","url":"/cohen-sutherland-clipping#id-2-5-segment-ij","position":15},{"hierarchy":{"lvl1":"Analyzing Cohen-Sutherland Algorithm","lvl2":"3. Synth√®se des r√©sultats"},"type":"lvl2","url":"/cohen-sutherland-clipping#id-3-synth-se-des-r-sultats","position":16},{"hierarchy":{"lvl1":"Analyzing Cohen-Sutherland Algorithm","lvl2":"3. Synth√®se des r√©sultats"},"content":"Segment\n\nCodes extr√©mit√©s\n\nD√©cision\n\nPortion dans le rectangle\n\n[AB]\n\n1001 ‚àß 1010 ‚â† 0\n\nrejet trivial\n\n‚Äî\n\n[CD]\n\n0000 ‚àß 0000 = 0\n\naccept trivial\n\n\\bigl[(5,6),(7,7)\\bigr]\n\n[EF]\n\n0101 ‚àß 0010 = 0\n\nclip (2 r√©flexions)\n\n\\bigl[(6.2,\\,4),\\,(10,\\,6.375)\\bigr]\n\n[GH]\n\n0000 ‚àß 0010 = 0\n\nclip (droite)\n\n\\bigl[(9,5),\\,(10,5)\\bigr]\n\n[IJ]\n\n0001 ‚àß 1000 = 0\n\nclip (gauche ‚Üí haut)\n\npoint unique (4,8) (segment d√©g√©n√©r√©)\n\nChaque √©tape de d√©coupage applique syst√©matiquement le premier bit non-nul (haut, bas, droite ou gauche), calcule l‚Äôintersection, met √† jour le point, puis passe √† l‚Äôautre extr√©mit√© tant que son code reste non nul.","type":"content","url":"/cohen-sutherland-clipping#id-3-synth-se-des-r-sultats","position":17},{"hierarchy":{"lvl1":"Solution pour l‚Äôexo avec methode du germe"},"type":"lvl1","url":"/methode-du-germe-1","position":0},{"hierarchy":{"lvl1":"Solution pour l‚Äôexo avec methode du germe"},"content":"\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2\n\n\n\n44\n\n45\n\n46\n\n47\n\n48\n\n49\n\n50\n\n51\n\n52\n\n\n\n\n\n3\n\n\n\n53\n\n54\n\n\n\n\n\n\n\n\n\n\n\n\n\n41\n\n42\n\n43\n\n4\n\n\n\n55\n\n56\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n39\n\n40\n\n5\n\n\n\n2\n\n3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n37\n\n38\n\n6\n\n\n\n4\n\n5\n\n\n\n\n\n\n\n\n\n\n\n34\n\n35\n\n36\n\n\n\n7\n\n\n\n6\n\n7\n\n8\n\n9\n\n10\n\n11\n\n12\n\n13\n\n14\n\n\n\n\n\n8\n\n\n\n26\n\n27\n\n\n\n\n\n\n\n15\n\n16\n\n17\n\n\n\n\n\n\n\n9\n\n\n\n28\n\n29\n\n\n\n\n\n\n\n\n\n18\n\n19\n\n20\n\n\n\n\n\n10\n\n\n\n30\n\n31\n\n\n\n\n\n\n\n\n\n\n\n21\n\n22\n\n23\n\n\n\n11\n\n\n\n32\n\n33\n\n\n\n\n\n\n\n\n\n\n\n\n\n24\n\n25\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the explanation of the algorithm, visit \n\nthis page.","type":"content","url":"/methode-du-germe-1","position":1},{"hierarchy":{"lvl1":"Projecting point onto the screen"},"type":"lvl1","url":"/temp-exo-proj","position":0},{"hierarchy":{"lvl1":"Projecting point onto the screen"},"content":"Exercise 2\n\nOn doit faire le rendu d‚Äôun point ( M(x_M, y_M, z_M) ) dans le rep√®re univers vers son point projet√© ( m(u_m, v_m) ) dans le rep√®re de l‚Äô√©cran 2D via une cam√©ra perspective ( C ). Identifier tous les param√®tres du syst√®me pour pouvoir effectuer ce rendu. D√©tailer sous forme de matrices de coordonn√©es homog√®nes l‚Äôensemble des transformations n√©cessaires pour le rendu. Il faudra donner simplement le produit des matrices canoniques sans en effectuer le calcul.\n\nOkay, let‚Äôs detail the process of rendering a 3D point from world space to 2D screen space using a perspective camera, drawing upon the provided sources.\n\nTo render a point M(x_M, y_M, z_M) in the ‚Äúrep√®re univers‚Äù (world space) to its projected point m(u_m, v_m) in the ‚Äúrep√®re de l‚Äô√©cran‚Äù (screen space) via a perspective camera C, we need to apply a sequence of geometric transformations as part of the graphics pipeline. These transformations change the coordinate system the point is defined in. The overall process can be described using homogeneous coordinates and matrix multiplications.\n\nHere are the essential parameters of the system required to perform this rendering:\n\nThe 3D Point M: The coordinates (x_M, y_M, z_M) of the point in the world coordinate system (rep√®re univers). This is the input to the rendering process.\n\nThe Perspective Camera C: The camera defines the viewpoint and how the 3D scene is projected onto a 2D image plane. Its parameters include:\n\nPosition: The location (T\\!x_C, T\\!y_C, T\\!z_C) of the camera‚Äôs origin in the world coordinate system.\n\nOrientation: The direction the camera is looking and its ‚Äúup‚Äù direction. This can be defined by a rotation R_C that transforms points from the camera‚Äôs local coordinate system to the world coordinate system.\n\nPerspective Projection Parameters: These define the view frustum, which is the region of space visible to the camera. For a perspective projection, this is typically a truncated pyramid. The sources define the frustum using the coordinates of its planes: left (l), right (r), bottom (b), top (t), near (n), and far (f). The near and far planes define the closest and farthest distances from the camera that objects are still rendered.\n\nThe Screen or Viewport: This defines the 2D rectangular area on the display device where the final image is drawn. Its parameters include:\n\nWidth (W) and Height (H): The dimensions of the viewport in pixels. This defines the mapping from the normalised projection space to the pixel coordinates on the screen.\n\nThe rendering process involves transforming the point M through several coordinate systems using matrix multiplications. Homogeneous coordinates, using 4√ó4 matrices for 3D points represented as 4D vectors (x, y, z, 1), allow translation, rotation, scaling, and perspective projection to all be performed via matrix multiplication.\n\nHere are the necessary transformations, step by step, detailed with homogeneous-coordinate matrices:","type":"content","url":"/temp-exo-proj","position":1},{"hierarchy":{"lvl1":"Projecting point onto the screen","lvl2":"Step 1: World to Camera Transformation (Viewing Transformation)"},"type":"lvl2","url":"/temp-exo-proj#step-1-world-to-camera-transformation-viewing-transformation","position":2},{"hierarchy":{"lvl1":"Projecting point onto the screen","lvl2":"Step 1: World to Camera Transformation (Viewing Transformation)"},"content":"This transformation changes the point‚Äôs coordinates from the world space to the camera‚Äôs local space (also called eye space or view space). This is equivalent to transforming the entire world such that the camera is positioned at the origin (0,0,0) and oriented along standard axes (e.g., looking down the negative Z‚Äëaxis).\n\nThe matrix performing this transformation, M_{\\mathrm{view}}, is the inverse of the matrix that transforms points from the camera‚Äôs coordinate system into the world coordinate system. If the camera‚Äôs position in world space is given by the translation vector T_C = (T\\!x_C, T\\!y_C, T\\!z_C) and its orientation by the rotation matrix R_C (which transforms camera‚Äëspace vectors to world‚Äëspace vectors), the transformation from world to camera space is given byM_{\\mathrm{view}} \\;=\\; R_C^T \\,\\cdot\\, T(-T_C).\n\nThe homogeneous translation matrix to move the world so the camera is at the origin is:T(-T_C) = \\begin{pmatrix}\n1 & 0 & 0 & -T\\!x_C \\\\\n0 & 1 & 0 & -T\\!y_C \\\\\n0 & 0 & 1 & -T\\!z_C \\\\\n0 & 0 & 0 & 1\n\\end{pmatrix}\n\nThe homogeneous rotation matrix R_C incorporates the camera‚Äôs orientation. Its transpose R_C^T performs the inverse rotation, aligning the world axes with the camera‚Äôs axes. If R_C is a 4√ó4 homogeneous rotation matrix (with the bottom‚Äëright element being 1 and the rest of the last row/column being 0 except for the 3√ó3 rotation part), then R_C^T is simply its transpose.\n\nThe World ‚Üí Camera transformation matrix is therefore:M_{\\mathrm{view}} = R_C^T \\,\\cdot\\, T(-T_C)\n\nThe point M in homogeneous world coordinates isP_{\\mathrm{world}} = \\begin{pmatrix}\nx_M \\\\\ny_M \\\\\nz_M \\\\\n1\n\\end{pmatrix}.\n\nThe point in camera‚Äëspace homogeneous coordinates isP_{\\mathrm{camera}} = M_{\\mathrm{view}} \\,\\cdot\\, P_{\\mathrm{world}}.","type":"content","url":"/temp-exo-proj#step-1-world-to-camera-transformation-viewing-transformation","position":3},{"hierarchy":{"lvl1":"Projecting point onto the screen","lvl2":"Step 2: Camera to Clip Space Transformation (Projection Transformation)"},"type":"lvl2","url":"/temp-exo-proj#step-2-camera-to-clip-space-transformation-projection-transformation","position":4},{"hierarchy":{"lvl1":"Projecting point onto the screen","lvl2":"Step 2: Camera to Clip Space Transformation (Projection Transformation)"},"content":"This transformation converts the point‚Äôs coordinates from the camera‚Äôs 3D space into a 3D space suitable for clipping and the subsequent perspective divide. It projects the 3D geometry onto a 2D plane while preserving depth information for visibility tests. For a perspective projection, this matrix implements the foreshortening effect (objects appearing smaller further away). The source material includes a specific matrix for the perspective projection that maps the view frustum (defined by l, r, b, t, n, f) into a Normalised Device Coordinates (NDC) cube, typically ranging from ‚Äì1 to 1 along each axis.\n\nThe full perspective projection matrix provided in the sources, mapping from camera space (view frustum) to NDC is:M_{\\mathrm{persp\\_proj}} = \\begin{pmatrix}\n\\frac{2}{r - l} & 0 & \\frac{l + r}{r - l} & 0 \\\\\n0 & \\frac{2}{t - b} & \\frac{b + t}{t - b} & 0 \\\\\n0 & 0 & -\\frac{f + n}{f - n} & -\\frac{2 f n}{f - n} \\\\\n0 & 0 & -1 & 0\n\\end{pmatrix}\n\nThe point in clip‚Äëspace homogeneous coordinates isP_{\\mathrm{clip}} = M_{\\mathrm{persp\\_proj}} \\,\\cdot\\, P_{\\mathrm{camera}}.\n\nLetP_{\\mathrm{clip}} = \\begin{pmatrix} X_c \\\\ Y_c \\\\ Z_c \\\\ W_c \\end{pmatrix}.\n\nStep 3: Perspective Division\n\nAfter the projection matrix is applied, the points are in homogeneous clip coordinates (X_c, Y_c, Z_c, W_c). The perspective effect is completed by dividing the X, Y, and Z components by the W component to get the 3D NDC coordinates:\n(x_{\\mathrm{ndc}},,y_{\\mathrm{ndc}},,z_{\\mathrm{ndc}})\\bigl(X_c/W_c,;Y_c/W_c,;Z_c/W_c\\bigr).","type":"content","url":"/temp-exo-proj#step-2-camera-to-clip-space-transformation-projection-transformation","position":5},{"hierarchy":{"lvl1":"Projecting point onto the screen","lvl2":"Step 4: Clipping"},"type":"lvl2","url":"/temp-exo-proj#step-4-clipping","position":6},{"hierarchy":{"lvl1":"Projecting point onto the screen","lvl2":"Step 4: Clipping"},"content":"Clipping is the process of removing geometry that lies outside the viewing volume (the frustum). After the perspective projection and division, points are in NDC space. The viewing volume in NDC is a cube defined by the ranges [-1,1] for x, y, and z. A point is visible if and only if-1 ,\\le, x_{\\mathrm{ndc}} ,\\le, 1,\\quad\n-1 ,\\le, y_{\\mathrm{ndc}} ,\\le, 1,\\quad\n-1 ,\\le, z_{\\mathrm{ndc}} ,\\le, 1.\n\nIf the point is outside this range, it is discarded from the pipeline; otherwise, it proceeds to the next stage.\n\nStep 5: NDC to Screen Transformation (Viewport Transformation)\n\nThis final transformation maps the point‚Äôs coordinates from the NDC space to the 2D screen space (pixel coordinates). The NDC coordinates range from ‚Äì1 to 1. The screen coordinates typically range from 0 to the screen width (W) for the x‚Äëaxis and 0 to the screen height (H) for the y‚Äëaxis. This transformation involves scaling and translating the NDC coordinates to fit the defined screen viewport. It also typically maps the NDC z coordinate (which is in [-1,1]) to a depth value (e.g., in [0,2^{\\mathrm{buffer_bits}}-1]) for use in the depth buffer (Z‚Äëbuffer) during the visibility/rendering step.\n\nThe homogeneous transformation matrix M_{\\mathrm{viewport}} mapping NDC [-1,1] to screen coordinates [0,W] for x and [0,H] for y (and [-1,1] to [0,1] for depth) is:M_{\\mathrm{viewport}} = \\begin{pmatrix}\nW/2 & 0   & 0   & W/2 \\\\\n0   & H/2 & 0   & H/2 \\\\\n0   & 0   & 1/2 & 1/2 \\\\\n0   & 0   & 0   & 1\n\\end{pmatrix}Note: The standard transformation maps NDC $z\\in[-1,1]$ to depth $\\in[0,1]$ for the Z‚Äëbuffer, which is why the third row uses a scale of 1/2 and translate of 1/2. For just the 2D screen coordinates $(u,v)$, only the first two rows are directly relevant after the final homogeneous division.\n\nThe point in screen‚Äëspace homogeneous coordinates (before the final division by W_s) is obtained by applying the viewport matrix to the clip‚Äëspace homogeneous point:P_{\\mathrm{screen\\_hom}} = M_{\\mathrm{viewport}} \\,\\cdot\\, P_{\\mathrm{clip}}\n\nLetP_{\\mathrm{screen\\_hom}} = \\begin{pmatrix}\nX_s \\\\ Y_s \\\\ Z_s \\\\ W_s\n\\end{pmatrix}.\n\nThe final 2D screen coordinates (u_m, v_m) are given by(u_m,,v_m) = \\bigl(X_s / W_s,;Y_s / W_s\\bigr).\n\nCombined Transformation Matrix\n\nThe complete transformation of the point M from world space to homogeneous screen space (before the final division) is achieved by multiplying the point‚Äôs homogeneous world coordinate vector by the sequence of transformation matrices:P_{\\mathrm{screen\\_hom}}\n= M_{\\mathrm{viewport}}\n  \\;\\cdot\\;\n  M_{\\mathrm{persp\\_proj}}\n  \\;\\cdot\\;\n  M_{\\mathrm{view}}\n  \\;\\cdot\\;\n  P_{\\mathrm{world}}.\n\nThe product of the canonical matrices performing the transformations from world space to homogeneous screen space is:M_{\\mathrm{total}}\n= M_{\\mathrm{viewport}}\n  \\;\\cdot\\;\n  M_{\\mathrm{persp\\_proj}}\n  \\;\\cdot\\;\n  M_{\\mathrm{view}}.M_{\\mathrm{total}}\n= \\begin{pmatrix}\nW/2 & 0   & 0   & W/2 \\\\\n0   & H/2 & 0   & H/2 \\\\\n0   & 0   & 1/2 & 1/2 \\\\\n0   & 0   & 0   & 1\n\\end{pmatrix}\n\\;\\cdot\\;\n\\begin{pmatrix}\n\\frac{2}{r - l} & 0 & \\frac{l + r}{r - l} & 0 \\\\\n0 & \\frac{2}{t - b} & \\frac{b + t}{t - b} & 0 \\\\\n0 & 0 & -\\frac{f + n}{f - n} & -\\frac{2 f n}{f - n} \\\\\n0 & 0 & -1 & 0\n\\end{pmatrix}\n\\;\\cdot\\;\n\\Bigl(\nR_C^T\n\\;\\cdot\\;\n\\begin{pmatrix}\n1 & 0 & 0 & -T\\!x_C \\\\\n0 & 1 & 0 & -T\\!y_C \\\\\n0 & 0 & 1 & -T\\!z_C \\\\\n0 & 0 & 0 & 1\n\\end{pmatrix}\n\\Bigr).\n\nTo get the final 2D screen coordinates (u_m, v_m), multiply P_{\\mathrm{world}} by M_{\\mathrm{total}} to get P_{\\mathrm{screen_hom}}=(X_s,Y_s,Z_s,W_s)^T, then perform the perspective divide:u_m = \\frac{X_s}{W_s}, \\quad v_m = \\frac{Y_s}{W_s}.\n\nThe value \\frac{Z_s}{W_s} can be used for depth testing.","type":"content","url":"/temp-exo-proj#step-4-clipping","position":7},{"hierarchy":{"lvl1":"Projecting point onto the screen","lvl2":"Why do matrices have exactly the form they do?"},"type":"lvl2","url":"/temp-exo-proj#why-do-matrices-have-exactly-the-form-they-do","position":8},{"hierarchy":{"lvl1":"Projecting point onto the screen","lvl2":"Why do matrices have exactly the form they do?"},"content":"That‚Äôs an excellent question, delving into the specifics of how these transformation matrices are constructed! The particular form and the coefficients (the numbers and letters) of the M_viewport and M_persp_proj matrices arise directly from their purpose in the graphics pipeline: mapping coordinates from one defined space to another. Let‚Äôs break them down based on the information in the sources.\n\nAs we established in our previous conversation and is outlined in the sources, rendering a 3D point involves a sequence of transformations: first moving from the object‚Äôs local space to the world space, then from world space to the camera‚Äôs space (the viewing transformation), then the projection (transforming 3D primitives onto a 2D image space), followed by clipping, and finally the transformation to screen space (the viewport transformation). Homogeneous coordinates and 4√ó4 matrices are used to unify these transformations.","type":"content","url":"/temp-exo-proj#why-do-matrices-have-exactly-the-form-they-do","position":9},{"hierarchy":{"lvl1":"Projecting point onto the screen","lvl3":"1. The Perspective Projection Matrix (M_persp_proj)","lvl2":"Why do matrices have exactly the form they do?"},"type":"lvl3","url":"/temp-exo-proj#id-1-the-perspective-projection-matrix-m-persp-proj","position":10},{"hierarchy":{"lvl1":"Projecting point onto the screen","lvl3":"1. The Perspective Projection Matrix (M_persp_proj)","lvl2":"Why do matrices have exactly the form they do?"},"content":"Purpose:\nThe perspective projection matrix transforms points from the camera‚Äôs 3D view space (eye space) into a standardised 3D space called Normalised Device Coordinates (NDC). This transformation is fundamental to creating the perspective effect, where objects appear smaller the further away they are. The sources mention that the projection process projects 3D primitives onto the 2D image space (screen space).\n\nView Frustum Parameters:\nFor a perspective projection, the camera sees a region of space called the view frustum. This frustum is shaped like a truncated pyramid. The sources define this frustum using six clipping planes: left (l), right (r), bottom (b), top (t), near (n), and far (f). The near plane (z-near) and far plane (z-far) define the boundaries along the depth axis. Objects outside this frustum are clipped (removed).\n\nMapping to NDC:\nThe goal of the perspective projection matrix is to map this view frustum into a canonical cube in NDC space. NDC coordinates typically range from -1 to 1 along each axis. The transformation ensures that points exactly on the near plane (z=-n in camera space) map to z\\_{\\text{ndc}}=-1, points exactly on the far plane (z=-f) map to z\\_{\\text{ndc}}=1, points on the left plane (x=l) map to x\\_{\\text{ndc}}=-1, points on the right plane (x=r) map to x\\_{\\text{ndc}}=1, points on the bottom plane (y=b) map to y\\_{\\text{ndc}}=-1, and points on the top plane (y=t) map to y\\_{\\text{ndc}}=1.\n\nThe Matrix Form:\nThe specific matrix provided in the sources achieves this mapping and prepares the coordinates for the perspective divide:\\mathbf{M_{persp\\_proj}}\n  = \n  \\begin{pmatrix}\n    \\displaystyle \\frac{2}{r - l} & 0 & \\displaystyle \\frac{l + r}{r - l} & 0 \\\\[1em]\n    0 & \\displaystyle \\frac{2}{t - b} & \\displaystyle \\frac{b + t}{t - b} & 0 \\\\[1em]\n    0 & 0 & -\\displaystyle \\frac{f + n}{f - n} & -\\displaystyle \\frac{2 f n}{f - n} \\\\[1em]\n    0 & 0 & -1 & 0\n  \\end{pmatrix}\n\nTop-Left 3√ó3 Block:\nHandles the scaling and mapping of the x, y, and z coordinates, while implicitly incorporating the perspective effect.\n\n\\frac{2}{r-l} and \\frac{l+r}{r-l} map the x-range [l,r] to [-1,1].\n\n\\frac{2}{t-b} and \\frac{b+t}{t-b} map the y-range [b,t] to [-1,1].\n\n-\\frac{f+n}{f-n} and -\\frac{2fn}{f-n} map the z-range [-f,-n] to [-1,1] (camera looks down ‚ÄìZ).\n\nBottom Row (0,0,-1,0):\nCauses the homogeneous w-coordinate to become -z\\_{\\text{camera}}, so that after multiplication by the matrix the resulting point (X\\_c, Y\\_c, Z\\_c, W\\_c) satisfies:x_{\\text{ndc}} = \\frac{X_c}{W_c} = \\frac{X_c}{-\\,z_{\\text{camera}}}, \n    \\quad\n    y_{\\text{ndc}} = \\frac{Y_c}{W_c} = \\frac{Y_c}{-\\,z_{\\text{camera}}},\n    \\quad\n    z_{\\text{ndc}} = \\frac{Z_c}{W_c}.\n\nDividing by -z\\_{\\text{camera}} implements the foreshortening perspective effect, and z\\_{\\text{ndc}} preserves depth for the Z-buffer.","type":"content","url":"/temp-exo-proj#id-1-the-perspective-projection-matrix-m-persp-proj","position":11},{"hierarchy":{"lvl1":"Projecting point onto the screen","lvl3":"2. The Viewport Transformation Matrix (M_viewport)","lvl2":"Why do matrices have exactly the form they do?"},"type":"lvl3","url":"/temp-exo-proj#id-2-the-viewport-transformation-matrix-m-viewport","position":12},{"hierarchy":{"lvl1":"Projecting point onto the screen","lvl3":"2. The Viewport Transformation Matrix (M_viewport)","lvl2":"Why do matrices have exactly the form they do?"},"content":"Purpose:\nThe viewport transformation is the final step before rendering to the screen. It takes the 3D point in NDC space [-1,1] (after the perspective divide and clipping) and maps its X and Y coordinates to the 2D pixel coordinates (u\\_m, v\\_m) on the screen. It also maps the NDC Z coordinate to a depth value suitable for the depth buffer.\n\nScreen Parameters:\nThe dimensions of the target display area, typically its Width (W) and Height (H) in pixels.\n\nMapping from NDC to Screen:\n\nu\\_m = x\\_{\\text{ndc}} \\cdot \\frac{W}{2} + \\frac{W}{2}\n\nv\\_m = y\\_{\\text{ndc}} \\cdot \\frac{H}{2} + \\frac{H}{2}\n\nz\\_{\\text{depth}} = z\\_{\\text{ndc}} \\cdot \\frac{1}{2} + \\frac{1}{2}\n\nThe Matrix Form:\nThese can be expressed as a homogeneous 4√ó4 matrix:\\mathbf{M_{viewport}} = \\begin{pmatrix}\n    \\tfrac{W}{2} & 0 & 0 & \\tfrac{W}{2} \\\\\n    0 & \\tfrac{H}{2} & 0 & \\tfrac{H}{2} \\\\\n    0 & 0 & \\tfrac{1}{2} & \\tfrac{1}{2} \\\\\n    0 & 0 & 0 & 1\n  \\end{pmatrix}\n\nWhen multiplied by (x\\_{\\text{ndc}}, y\\_{\\text{ndc}}, z\\_{\\text{ndc}}, 1), the first row yields u\\_m, the second v\\_m, and the third z\\_{\\text{depth}}.\n\nIn summary, the specific forms and coefficients of these matrices are not arbitrary. They are carefully constructed algebraic representations of the geometric transformations required to map coordinates from one space to another‚Äîfrustum to NDC, and NDC to screen‚Äîimplementing the precise scaling, translation, and perspective‚Äêdivision effects that define the graphics pipeline. The parameters (l, r, b, t, n, f, W, H) define the geometry of the input (frustum) and output (screen) spaces, and the matrix coefficients are derived to perform the exact mapping between them.","type":"content","url":"/temp-exo-proj#id-2-the-viewport-transformation-matrix-m-viewport","position":13},{"hierarchy":{"lvl1":"Transformation matrix for the rotation around the line from the given points"},"type":"lvl1","url":"/transformation-matrix-for-turning","position":0},{"hierarchy":{"lvl1":"Transformation matrix for the rotation around the line from the given points"},"content":"Question 3\n\nSoit l‚Äôaxe \\vec{d} d√©fini par les deux points P_1\\,(1,\\,2,\\,-4) et P_2\\,(3,\\,1,\\,7). D√©finir la matrice de transformation de rotation de 45^\\circ autour de l‚Äôaxe \\vec{d}. On exprimera cette matrice sous forme d‚Äôun produit de matrices canoniques. Il n‚Äôest pas n√©cessaire d‚Äôeffectuer les multiplications explicites.\n\nD√©taillons chacune des √©tapes, en justifiant leur r√¥le, en expliquant comment fonctionnent les matrices et ce que repr√©sentent Œ±, Œ≤, etc.","type":"content","url":"/transformation-matrix-for-turning","position":1},{"hierarchy":{"lvl1":"Transformation matrix for the rotation around the line from the given points","lvl2":"1. Trouver le vecteur directeur \\vec u de l‚Äôaxe"},"type":"lvl2","url":"/transformation-matrix-for-turning#id-1-trouver-le-vecteur-directeur-vec-u-de-laxe","position":2},{"hierarchy":{"lvl1":"Transformation matrix for the rotation around the line from the given points","lvl2":"1. Trouver le vecteur directeur \\vec u de l‚Äôaxe"},"content":"On a deux points P_1=(1,2,-4) et P_2=(3,1,7).\n\nLe vecteur directeur \\vec u=P_2 - P_1 pointe dans la direction de l‚Äôaxe :\\vec u = (3-1,\\;1-2,\\;7-(-4)) = (2,\\,-1,\\;11).\n\nPourquoi ?\nPour d√©finir une droite en 3D, on a besoin\n\nd‚Äôun point P_1 qu‚Äôelle contient,\n\nd‚Äôune direction \\vec u.\nCe vecteur sert √† orienter toutes nos rotations ult√©rieures.","type":"content","url":"/transformation-matrix-for-turning#id-1-trouver-le-vecteur-directeur-vec-u-de-laxe","position":3},{"hierarchy":{"lvl1":"Transformation matrix for the rotation around the line from the given points","lvl2":"2. Translation pour poser P_1 √† l‚Äôorigine"},"type":"lvl2","url":"/transformation-matrix-for-turning#id-2-translation-pour-poser-p-1-lorigine","position":4},{"hierarchy":{"lvl1":"Transformation matrix for the rotation around the line from the given points","lvl2":"2. Translation pour poser P_1 √† l‚Äôorigine"},"content":"Matrice de translation de -P_1 :T_{-P_1} = \n    \\begin{pmatrix}\n      1 & 0 & 0 & -1\\\\\n      0 & 1 & 0 & -2\\\\\n      0 & 0 & 1 & +4\\\\\n      0 & 0 & 0 & 1\n    \\end{pmatrix}.\n\nAppliqu√©e √† un point homog√®ne \\bigl[x,y,z,1\\bigr]^T, on obtient (x-1,y-2,z+4).\n\nPourquoi ?\nLes rotations que nous connaissons (autour de Ox,Oy,Oz) sont centr√©es √† l‚Äôorigine.\nEn ‚Äúramenant‚Äù P_1 √† (0,0,0), on pourra aligner et faire tourner l‚Äôaxe proprement autour de l‚Äôorigine.","type":"content","url":"/transformation-matrix-for-turning#id-2-translation-pour-poser-p-1-lorigine","position":5},{"hierarchy":{"lvl1":"Transformation matrix for the rotation around the line from the given points","lvl2":"3. Rotation autour de Oz pour annuler la composante y de \\vec u"},"type":"lvl2","url":"/transformation-matrix-for-turning#id-3-rotation-autour-de-oz-pour-annuler-la-composante-y-de-vec-u","position":6},{"hierarchy":{"lvl1":"Transformation matrix for the rotation around the line from the given points","lvl2":"3. Rotation autour de Oz pour annuler la composante y de \\vec u"},"content":"Projection dans le plan xy :\n\\pi_{xy}(\\vec u) = (u_x,u_y) = (2,-1).\n\nAngle Œ± :\\alpha = \\arg(2 - i\\,1) = \\arctan2(u_y,u_x) = \\arctan2(-1,2).\n\nNum√©riquement, \\alpha \\approx -26{,}6^\\circ.\n\nRotation inverse R_z(-\\alpha) :R_z(-\\alpha)\n     = \\begin{pmatrix}\n         \\cos(-\\alpha) & -\\sin(-\\alpha) & 0 \\\\\n         \\sin(-\\alpha) &  \\cos(-\\alpha) & 0 \\\\\n         0             &  0             & 1\n       \\end{pmatrix}.\n\nApr√®s cette rotation, le vecteur \\vec u devientR_z(-\\alpha)\\,\\vec u\n     = \\bigl(\\sqrt{2^2+(-1)^2},\\,0,\\,11\\bigr) = (\\sqrt5,\\,0,\\,11).\n\nPourquoi ?\nOn ‚Äúpivote‚Äù autour de l‚Äôaxe z pour faire dispara√Ætre la composante y :\nla projection de \\vec u tombe alors sur l‚Äôaxe x.\nCela simplifie l‚Äô√©tape suivante car on n‚Äôa plus qu‚Äôun plan xz.","type":"content","url":"/transformation-matrix-for-turning#id-3-rotation-autour-de-oz-pour-annuler-la-composante-y-de-vec-u","position":7},{"hierarchy":{"lvl1":"Transformation matrix for the rotation around the line from the given points","lvl2":"4. Rotation autour de Oy pour aligner enti√®rement sur Oz"},"type":"lvl2","url":"/transformation-matrix-for-turning#id-4-rotation-autour-de-oy-pour-aligner-enti-rement-sur-oz","position":8},{"hierarchy":{"lvl1":"Transformation matrix for the rotation around the line from the given points","lvl2":"4. Rotation autour de Oy pour aligner enti√®rement sur Oz"},"content":"Longueur dans le plan xz :r = \\sqrt{(\\sqrt5)^2 + 0^2} = \\sqrt5.\n\nAngle Œ≤ :\\beta = \\arctan\\!\\frac{r}{u_z} = \\arctan\\!\\frac{\\sqrt5}{11}.\n\nC‚Äôest l‚Äôangle entre le vecteur (r,0,11) et l‚Äôaxe Oz.\n\nRotation inverse R_y(-\\beta) :R_y(-\\beta)\n     = \\begin{pmatrix}\n         \\cos(-\\beta) & 0 & \\sin(-\\beta) \\\\\n         0            & 1 & 0            \\\\\n        -\\sin(-\\beta)& 0 & \\cos(-\\beta)\n       \\end{pmatrix}.\n\nElle envoie \\bigl(r,0,11\\bigr) sur \\bigl(0,0,\\sqrt5^2+11^2\\bigr) = (0,0,\\|\\vec u\\|).\n\nPourquoi ?\nOn tourne autour de l‚Äôaxe y pour ‚Äúredresser‚Äù le vecteur \\vec u sur l‚Äôaxe z.\nApr√®s R_z(-\\alpha) et R_y(-\\beta), l‚Äôaxe \\vec d co√Øncide exactement avec Oz.","type":"content","url":"/transformation-matrix-for-turning#id-4-rotation-autour-de-oy-pour-aligner-enti-rement-sur-oz","position":9},{"hierarchy":{"lvl1":"Transformation matrix for the rotation around the line from the given points","lvl2":"5. Rotation d‚Äôangle \\theta=45^\\circ autour de l‚Äôaxe Oz"},"type":"lvl2","url":"/transformation-matrix-for-turning#id-5-rotation-dangle-theta-45-circ-autour-de-laxe-oz","position":10},{"hierarchy":{"lvl1":"Transformation matrix for the rotation around the line from the given points","lvl2":"5. Rotation d‚Äôangle \\theta=45^\\circ autour de l‚Äôaxe Oz"},"content":"Matrice :R_z(\\theta)\n    = \\begin{pmatrix}\n        \\cos\\tfrac\\pi4 & -\\sin\\tfrac\\pi4 & 0 \\\\\n        \\sin\\tfrac\\pi4 &  \\cos\\tfrac\\pi4 & 0 \\\\\n        0               &  0               & 1\n      \\end{pmatrix},\n    \\quad \\theta = 45^\\circ.\n\nR√¥le :\nC‚Äôest la seule rotation ‚Äúeffective‚Äù : elle fait tourner l‚Äôespace de 45^\\circ autour de Oz (donc autour de notre axe \\vec d, maintenant align√©).","type":"content","url":"/transformation-matrix-for-turning#id-5-rotation-dangle-theta-45-circ-autour-de-laxe-oz","position":11},{"hierarchy":{"lvl1":"Transformation matrix for the rotation around the line from the given points","lvl2":"6. D√©faire les transformations d‚Äôalignement"},"type":"lvl2","url":"/transformation-matrix-for-turning#id-6-d-faire-les-transformations-dalignement","position":12},{"hierarchy":{"lvl1":"Transformation matrix for the rotation around the line from the given points","lvl2":"6. D√©faire les transformations d‚Äôalignement"},"content":"Pour ramener l‚Äôaxe et l‚Äôorigine dans leur position initiale, on applique, dans l‚Äôordre inverse, les inverses des √©tapes 3‚Äì2 :\n\nR_y(\\beta) : inverse de R_y(-\\beta).\n\nR_z(\\alpha) : inverse de R_z(-\\alpha).\n\nT_{P_1} : translation de vecteur (1,2,-4), inverse de T_{-P_1}.","type":"content","url":"/transformation-matrix-for-turning#id-6-d-faire-les-transformations-dalignement","position":13},{"hierarchy":{"lvl1":"Transformation matrix for the rotation around the line from the given points","lvl2":"7. Composition finale"},"type":"lvl2","url":"/transformation-matrix-for-turning#id-7-composition-finale","position":14},{"hierarchy":{"lvl1":"Transformation matrix for the rotation around the line from the given points","lvl2":"7. Composition finale"},"content":"En coordonn√©es homog√®nes, la matrice de la rotation cherch√©e vaut donc, en appliquant de droite √† gauche :\\boxed{\nM \\;=\\;\nT_{P_1}\n\\;\\;\nR_z(\\alpha)\n\\;\\;\nR_y(\\beta)\n\\;\\;\nR_z\\bigl(45^\\circ\\bigr)\n\\;\\;\nR_y(-\\beta)\n\\;\\;\nR_z(-\\alpha)\n\\;\\;\nT_{-P_1}.\n}\n\nInterpr√©tation :\n\nTranslation pour centrer P_1.\n\nDeux rotations √©l√©mentaires pour aligner l‚Äôaxe arbitraire sur Oz.\n\nRotation de 45^\\circ autour de cet axe.\n\nInversion des deux rotations d‚Äôalignement.\n\nTranslation inverse.\n\nPourquoi laisser sous forme de produit ?\nCela garde la structure g√©om√©trique claire :\n\nchaque matrice est simple,\n\non peut changer facilement l‚Äôangle, ou le point P_1,\n\naucun calcul matriciel lourd n‚Äôest n√©cessaire pour comprendre la construction.\n\nR√©capitulatif des symboles\n\nT_{-P_1},\\,T_{P_1} : translations.\n\nR_z(\\phi) : rotation‚Äê√©l√©mentaire autour de l‚Äôaxe z d‚Äôangle œï.\n\nR_y(\\psi) : rotation‚Äê√©l√©mentaire autour de l‚Äôaxe y d‚Äôangle œà.\n\n\\alpha = \\arctan2(u_y,u_x) : angle de la projection dans le plan xy.\n\n\\beta = \\arctan\\!\\tfrac{\\sqrt{u_x^2+u_y^2}}{u_z} : angle pour amener le vecteur sur Oz.\n\n\\theta=45^\\circ : l‚Äôangle de rotation final autour de l‚Äôaxe donn√©.\n\nChaque √©tape sert √† transformer un probl√®me de rotation autour d‚Äôun axe arbitraire en une rotation autour d‚Äôun axe coordonn√© plus simple, puis √† revenir √† la configuration initiale.","type":"content","url":"/transformation-matrix-for-turning#id-7-composition-finale","position":15},{"hierarchy":{"lvl1":"Lecture: Illumination and Textures"},"type":"lvl1","url":"/lecture-4-5-gemini","position":0},{"hierarchy":{"lvl1":"Lecture: Illumination and Textures"},"content":"This lecture covers the crucial stages in the graphics pipeline responsible for determining the colour and visual appearance of 3D objects: Illumination (Shading) and Texture Mapping. These processes occur after the modelling transformations and before clipping, projection, rasterisation, and visibility determination.","type":"content","url":"/lecture-4-5-gemini","position":1},{"hierarchy":{"lvl1":"Lecture: Illumination and Textures","lvl2":"I. Illumination (√âclairage)"},"type":"lvl2","url":"/lecture-4-5-gemini#i-illumination-clairage","position":2},{"hierarchy":{"lvl1":"Lecture: Illumination and Textures","lvl2":"I. Illumination (√âclairage)"},"content":"Illumination refers to the process of calculating the transport of luminous flux, both directly and indirectly, from light sources within a scene. It can be categorised as local or global. This lecture primarily focuses on local illumination models, where the lighting at a surface point is calculated based only on the properties of the object at that point, the light sources, and the viewer‚Äôs position, without considering light interactions between different objects (like shadows or reflections).\n\n√âclairement (Illuminance) is the calculation of the luminous intensity at a specific point on a surface in the scene. It involves a model of interaction between a light source and the illuminated point.\n\nThe illumination at a point depends on several factors:\n\nPosition of the point in space.\n\nOrientation of the point (surface normal).\n\nCharacteristics of the surface (e.g., how much it diffuses or reflects light, its transparency).\n\nSources of light (their type, position, direction, intensity).\n\nPosition and orientation of the ‚Äúcamera‚Äù or viewpoint.","type":"content","url":"/lecture-4-5-gemini#i-illumination-clairage","position":3},{"hierarchy":{"lvl1":"Lecture: Illumination and Textures","lvl3":"A. Sources of Light","lvl2":"I. Illumination (√âclairage)"},"type":"lvl3","url":"/lecture-4-5-gemini#a-sources-of-light","position":4},{"hierarchy":{"lvl1":"Lecture: Illumination and Textures","lvl3":"A. Sources of Light","lvl2":"I. Illumination (√âclairage)"},"content":"Different types of light sources are used to simulate various lighting conditions:\n\nAmbient Light: This light illuminates the entire scene uniformly from all directions. It is the simplest model, characterised only by its intensity. It‚Äôs often used to provide a minimum level of lighting to prevent objects in shadow from being completely black. However, it does not convey any sense of 3D form.\n\nPoint Sources: These light sources are located at a specific point in 3D space and radiate light radially in all directions. They are characterised by their intensity, position, and a falloff function that determines how the light intensity decreases with distance. Point sources can be isotropic (radiating equally in all directions) or anisotropic (radiating more strongly in some directions). When a point source has volume, it becomes an extended source.\n\nDirectional Sources: These sources are assumed to be infinitely far away and illuminate the scene with parallel rays in a given direction. They are characterised by their intensity and direction. A common example is sunlight.\n\nSpot Sources (Projector Lights): These sources are defined by their position, direction, and a concentration factor that controls the focus of the light beam. They emit light in a cone along their specified direction.","type":"content","url":"/lecture-4-5-gemini#a-sources-of-light","position":5},{"hierarchy":{"lvl1":"Lecture: Illumination and Textures","lvl3":"B. Models of Illumination (√âclairement)","lvl2":"I. Illumination (√âclairage)"},"type":"lvl3","url":"/lecture-4-5-gemini#b-models-of-illumination-clairement","position":6},{"hierarchy":{"lvl1":"Lecture: Illumination and Textures","lvl3":"B. Models of Illumination (√âclairement)","lvl2":"I. Illumination (√âclairage)"},"content":"Local illumination models calculate the light intensity at a point by considering different components:\n\nEmitted Light: This represents light that an object inherently produces. In many standard rendering scenarios, objects are not intrinsic emitters of light and thus do not illuminate other objects. However, they may have a minimum level of self-illumination.\n\nAmbient Light: As discussed with light sources, the ambient illumination component calculates the contribution of a uniform, omnidirectional light source. The intensity at a point due to ambient light (I_a) is calculated as:\nI_a = p_a \\cdot I_{La}\nwhere p_a is the ambient reflection coefficient of the surface material and I_{La} is the intensity of the ambient light source. This is often calculated per colour component.\n\nDiffuse Reflection: This models light that is scattered equally in all directions from a surface. The intensity of diffuse reflection (I_d) at a point depends on the angle (Œ∏) between the incident light ray (\\mathbf{L}) and the surface normal (\\mathbf{N}) at that point. The formula is based on Lambert‚Äôs Law:\nI_d = p_d \\cdot I_L \\cdot \\cos(\\theta) = p_d \\cdot I_L \\cdot (\\mathbf{N} \\cdot \\mathbf{L})\nwhere p_d is the diffuse reflection coefficient of the surface material and I_L is the intensity of the light source. \\mathbf{N} and \\mathbf{L} are normalised vectors. The dot product (\\mathbf{N} \\cdot \\mathbf{L}) is clamped to zero to handle cases where the light is behind the surface (\\theta > 90^\\circ).\n\nSpecular Reflection (Brillance): This models the mirror-like reflection of light from a surface, resulting in highlights. The intensity of specular reflection is highly view-dependent. Two common models are:\n\nPhong Model (1973): This model calculates a reflection vector (\\mathbf{R}) by reflecting the light vector \\mathbf{L} across the surface normal \\mathbf{N}. The specular intensity (I_s) depends on the angle (\\theta') between the reflection vector \\mathbf{R} and the viewing direction vector (\\mathbf{V}):\nI_s = p_s \\cdot I_L \\cdot \\cos(\\theta')^n = p_s \\cdot I_L \\cdot (\\mathbf{R} \\cdot \\mathbf{V})^n\nwhere p_s is the specular reflection coefficient, I_L is the light intensity, and n is the shininess exponent (or roughness), which controls the size and sharpness of the specular highlight. A higher n results in a smaller, more intense highlight (simulating a smoother surface), while a lower n produces a larger, more diffuse highlight (rougher surface).\n\nBlinn-Phong Model (1977): This is a more efficient approximation of the Phong model. Instead of calculating the reflection vector \\mathbf{R}, it introduces a half-vector (\\mathbf{H}) which is the normalised sum of the light vector \\mathbf{L} and the view vector \\mathbf{V}:\n\\mathbf{H} = \\frac{\\mathbf{L} + \\mathbf{V}}{||\\mathbf{L} + \\mathbf{V}||}\nThe specular intensity is then calculated based on the angle (\\theta'') between the half-vector \\mathbf{H} and the surface normal \\mathbf{N}:\nI_s = p_s \\cdot I_L \\cdot \\cos(\\theta'')^n = p_s \\cdot I_L \\cdot (\\mathbf{N} \\cdot \\mathbf{H})^n\nThis avoids the relatively expensive reflection vector calculation.\n\nThe complete local illumination model often combines these components, possibly with an attenuation factor (F_d) to account for the distance between the light source and the illuminated point:\n\nI(P) = p_a \\cdot I_a + F_d \\cdot (p_d \\cdot I_L \\cdot (\\mathbf{N} \\cdot \\mathbf{L}) + p_s \\cdot I_L \\cdot (\\mathbf{R/H} \\cdot \\mathbf{V/N})^n )","type":"content","url":"/lecture-4-5-gemini#b-models-of-illumination-clairement","position":7},{"hierarchy":{"lvl1":"Lecture: Illumination and Textures","lvl3":"C. Colour, Transparency, and Halos","lvl2":"I. Illumination (√âclairage)"},"type":"lvl3","url":"/lecture-4-5-gemini#c-colour-transparency-and-halos","position":8},{"hierarchy":{"lvl1":"Lecture: Illumination and Textures","lvl3":"C. Colour, Transparency, and Halos","lvl2":"I. Illumination (√âclairage)"},"content":"Illumination models are often applied to each colour component (Red, Green, Blue) separately, using the material properties and light source intensities for each component.\n\nTransparency can be incorporated by blending the calculated colour of the object with the colour of the background behind it. A transparency parameter (t) determines the weight of each colour:\n\nI = t \\cdot I(P)_{object} + (1-t) \\cdot I_{background}\n\nHalos (or transmission effects) can occur with transparent objects where the colour depends on the thickness of the material the light passes through.","type":"content","url":"/lecture-4-5-gemini#c-colour-transparency-and-halos","position":9},{"hierarchy":{"lvl1":"Lecture: Illumination and Textures","lvl2":"II. Shading (Ombrage)"},"type":"lvl2","url":"/lecture-4-5-gemini#ii-shading-ombrage","position":10},{"hierarchy":{"lvl1":"Lecture: Illumination and Textures","lvl2":"II. Shading (Ombrage)"},"content":"Shading is the process of using the illumination model to determine the colour of each pixel that represents a 3D surface. Different shading techniques exist, which affect the smoothness and realism of the rendered surfaces:","type":"content","url":"/lecture-4-5-gemini#ii-shading-ombrage","position":11},{"hierarchy":{"lvl1":"Lecture: Illumination and Textures","lvl3":"A. Local Shading Models","lvl2":"II. Shading (Ombrage)"},"type":"lvl3","url":"/lecture-4-5-gemini#a-local-shading-models","position":12},{"hierarchy":{"lvl1":"Lecture: Illumination and Textures","lvl3":"A. Local Shading Models","lvl2":"II. Shading (Ombrage)"},"content":"These models calculate the luminance at a surface point based on the object‚Äôs parameters and the light source parameters.\n\nFlat Shading: This is the simplest shading method where a single colour and intensity are calculated for an entire polygon (face). The normal vector for the entire face is used in the illumination calculations. This results in a faceted appearance for curved surfaces.\n\nLambert Shading: This specifically refers to applying only the diffuse reflection component (based on Lambert‚Äôs Law) uniformly across a polygon. Like flat shading, it uses a single normal for the entire face and does not produce highlights.\n\nGouraud Shading (1971): This technique aims to eliminate the intensity discontinuities across polygonal faces by interpolating the light intensities calculated at the vertices of the polygon.\n\nCalculate the surface normal at each vertex, often by averaging the normals of the faces sharing that vertex.\n\nApply the chosen illumination model at each vertex to calculate an intensity (or colour).\n\nDuring rasterisation, linearly interpolate these vertex intensities across the edges of the polygon and then between the edges for each scan line.\nGouraud shading is efficient and widely used, providing a smoother appearance than flat shading. However, it can miss specular highlights that fall in the middle of a polygon and can produce Mach banding effects in areas of rapidly changing intensity.\n\nPhong Shading (1973): This method provides even smoother shading and can handle specular highlights more effectively than Gouraud shading. Instead of interpolating intensities, Phong shading interpolates the normal vectors at the vertices across the polygon.\n\nCalculate the surface normal at each vertex.\n\nDuring rasterisation, linearly interpolate these vertex normals across the edges and then across the scan lines of the polygon.\n\nAt each pixel within the polygon, normalise the interpolated normal and then apply the chosen illumination model to calculate the final colour.\nPhong shading generally produces more realistic results, especially for specular reflections, as the highlights can appear within the faces of polygons. However, it is computationally more expensive than Gouraud shading because the illumination model needs to be evaluated at each pixel.","type":"content","url":"/lecture-4-5-gemini#a-local-shading-models","position":13},{"hierarchy":{"lvl1":"Lecture: Illumination and Textures","lvl2":"III. Textures (Plaquage, Mappage)"},"type":"lvl2","url":"/lecture-4-5-gemini#iii-textures-plaquage-mappage","position":14},{"hierarchy":{"lvl1":"Lecture: Illumination and Textures","lvl2":"III. Textures (Plaquage, Mappage)"},"content":"Texture mapping is a technique used to add fine surface detail to 3D objects without increasing the geometric complexity (number of polygons). Instead of defining colour and other attributes per vertex, an image (the texture) is ‚Äúglued‚Äù or mapped onto the surface of the object.","type":"content","url":"/lecture-4-5-gemini#iii-textures-plaquage-mappage","position":15},{"hierarchy":{"lvl1":"Lecture: Illumination and Textures","lvl3":"A. Texture Coordinates","lvl2":"III. Textures (Plaquage, Mappage)"},"type":"lvl3","url":"/lecture-4-5-gemini#a-texture-coordinates","position":16},{"hierarchy":{"lvl1":"Lecture: Illumination and Textures","lvl3":"A. Texture Coordinates","lvl2":"III. Textures (Plaquage, Mappage)"},"content":"To map a texture onto an object, texture coordinates (typically denoted as (u, v)) are associated with each vertex of the 3D model. These coordinates range from 0 to 1 (or sometimes outside this range for tiling or repeating textures) and define how the texture image is stretched or compressed to fit the object‚Äôs surface. During rasterisation, these texture coordinates are interpolated across the surface of the polygon for each fragment (pixel).","type":"content","url":"/lecture-4-5-gemini#a-texture-coordinates","position":17},{"hierarchy":{"lvl1":"Lecture: Illumination and Textures","lvl3":"B. Types of Textures","lvl2":"III. Textures (Plaquage, Mappage)"},"type":"lvl3","url":"/lecture-4-5-gemini#b-types-of-textures","position":18},{"hierarchy":{"lvl1":"Lecture: Illumination and Textures","lvl3":"B. Types of Textures","lvl2":"III. Textures (Plaquage, Mappage)"},"content":"Various types of textures can be used to control different surface properties:\n\nColour Map (Diffuse Map, Albedo): This texture directly defines the base colour of the object‚Äôs surface.\n\nTransparency Map (Alpha Channel): This texture controls the opacity of the object, allowing for see-through effects.\n\nBump Maps (Normal Maps, Displacement Maps, Height Maps): These textures are used to simulate surface relief and detail without altering the underlying geometry.\n\nBump Maps perturb the surface normal at each point based on the texture value, affecting how light reflects and creating the illusion of bumps and dents.\n\nNormal Maps directly store the perturbed normal vectors in the texture, providing a more accurate way to simulate surface detail.\n\nDisplacement Maps actually displace the vertices of the geometry based on the texture value, changing the object‚Äôs shape.\n\nSpecular Maps (Specular Intensity, Shininess, Roughness, Metallic): These textures control the properties of specular reflections, such as their colour, intensity, and size.\n\nEnvironment Maps (Cube Maps, Spherical Maps): These textures store the surrounding environment and are used to simulate reflections on shiny surfaces.\n\nLight Maps: These textures store pre-calculated illumination information (e.g., shadows, global illumination) to reduce the computational cost of real-time rendering.\n\nOthers: Various other texture types exist for specialised effects, such as occlusion maps, emissive maps, subsurface scattering maps, and more.","type":"content","url":"/lecture-4-5-gemini#b-types-of-textures","position":19},{"hierarchy":{"lvl1":"Lecture: Illumination and Textures","lvl3":"C. Texture Mapping Process and Issues","lvl2":"III. Textures (Plaquage, Mappage)"},"type":"lvl3","url":"/lecture-4-5-gemini#c-texture-mapping-process-and-issues","position":20},{"hierarchy":{"lvl1":"Lecture: Illumination and Textures","lvl3":"C. Texture Mapping Process and Issues","lvl2":"III. Textures (Plaquage, Mappage)"},"content":"During rendering, for each fragment (pixel) covered by a textured polygon, the interpolated (u, v) texture coordinates are used to look up the corresponding colour or attribute value from the texture image.\n\nHowever, several issues can arise during texture mapping:\n\nMagnification (Camera close to the object): When a single pixel on the screen corresponds to a very small area of the texture, the texture may appear blocky or pixelated. Filtering techniques are used to interpolate between texels (texture pixels) to produce a smoother result.\n\nMinification (Camera far from the object): When a single pixel on the screen covers a large area of the texture, simply sampling a single texel can lead to aliasing artefacts like jagged edges or moir√© patterns. This is because high-frequency details in the texture are undersampled.","type":"content","url":"/lecture-4-5-gemini#c-texture-mapping-process-and-issues","position":21},{"hierarchy":{"lvl1":"Lecture: Illumination and Textures","lvl3":"D. Antialiasing for Textures","lvl2":"III. Textures (Plaquage, Mappage)"},"type":"lvl3","url":"/lecture-4-5-gemini#d-antialiasing-for-textures","position":22},{"hierarchy":{"lvl1":"Lecture: Illumination and Textures","lvl3":"D. Antialiasing for Textures","lvl2":"III. Textures (Plaquage, Mappage)"},"content":"Several techniques are used to mitigate texture aliasing:\n\nSupersampling: Rendering the image at a higher resolution and then downsampling (averaging) to the target resolution. This is computationally expensive.\n\nMipmapping: Creating pre-filtered, lower-resolution versions of the texture image. The appropriate mipmap level is chosen based on the distance and angle at which the texture is viewed, reducing aliasing and improving performance.\n\nAnisotropic Filtering: A more sophisticated filtering technique that reduces aliasing better than mipmapping, especially for surfaces viewed at oblique angles.\n\nThese lecture notes provide a detailed overview of the concepts of illumination (shading) and texture mapping as presented in the ‚Äú4-5 lecture‚Äù. Understanding these concepts is fundamental to creating visually appealing and realistic 3D graphics, as they determine how light interacts with surfaces and how surface detail is represented.","type":"content","url":"/lecture-4-5-gemini#d-antialiasing-for-textures","position":23},{"hierarchy":{"lvl1":"Chapter 6: Snipping Away the Unseen - The Art of Clipping"},"type":"lvl1","url":"/lecture-6-gemini","position":0},{"hierarchy":{"lvl1":"Chapter 6: Snipping Away the Unseen - The Art of Clipping"},"content":"","type":"content","url":"/lecture-6-gemini","position":1},{"hierarchy":{"lvl1":"Chapter 6: Snipping Away the Unseen - The Art of Clipping"},"type":"lvl1","url":"/lecture-6-gemini#chapter-6-snipping-away-the-unseen-the-art-of-clipping","position":2},{"hierarchy":{"lvl1":"Chapter 6: Snipping Away the Unseen - The Art of Clipping"},"content":"Welcome, intrepid explorer of the digital canvas, to a crucial stage in our journey through the realm of computer graphics: Clipping. Up until now, we‚Äôve conjured 3D scenes filled with objects, adorned them with materials and bathed them in light, and even positioned our virtual camera to frame the view. But what happens to the parts of our meticulously crafted world that fall outside the camera‚Äôs gaze? Do we waste precious computational resources on them? The answer, thankfully, is no. This is where the elegant process of clipping steps in, acting as a discerning gatekeeper, ensuring that only what is visible makes its way further down the graphics pipeline.","type":"content","url":"/lecture-6-gemini#chapter-6-snipping-away-the-unseen-the-art-of-clipping","position":3},{"hierarchy":{"lvl1":"Chapter 6: Snipping Away the Unseen - The Art of Clipping","lvl2":"Defining the Boundaries: What is Clipping?"},"type":"lvl2","url":"/lecture-6-gemini#defining-the-boundaries-what-is-clipping","position":4},{"hierarchy":{"lvl1":"Chapter 6: Snipping Away the Unseen - The Art of Clipping","lvl2":"Defining the Boundaries: What is Clipping?"},"content":"At its heart, clipping is the process of determining which parts of a graphical object lie within the viewing region and discarding or modifying the parts that lie outside. Think of it like taking a photograph through a window. The window frame defines the boundaries of your view; anything outside this frame is not captured in the image. In computer graphics, the ‚Äúwindow‚Äù is typically the viewing frustum in 3D space, which gets projected onto a 2D clip window or viewport on the screen.\n\nThe primary goal of clipping is two-fold:\n\nEfficiency: By removing invisible geometry early in the pipeline, we avoid performing unnecessary calculations for transformations, illumination (shading), and rasterisation on parts of the scene that will never be displayed. This significantly improves rendering performance.\n\nVisual Coherence: Clipping ensures that only the relevant portions of objects are drawn, preventing artifacts and maintaining a visually consistent scene.","type":"content","url":"/lecture-6-gemini#defining-the-boundaries-what-is-clipping","position":5},{"hierarchy":{"lvl1":"Chapter 6: Snipping Away the Unseen - The Art of Clipping","lvl2":"Why Bother? The Necessity of Clipping"},"type":"lvl2","url":"/lecture-6-gemini#why-bother-the-necessity-of-clipping","position":6},{"hierarchy":{"lvl1":"Chapter 6: Snipping Away the Unseen - The Art of Clipping","lvl2":"Why Bother? The Necessity of Clipping"},"content":"Imagine rendering a vast landscape. Without clipping, your graphics system would have to process every tree, every hill, even the distant mountains that are only a few pixels on the horizon (or completely obscured). This would be incredibly inefficient. Clipping allows us to focus our efforts on the geometry that actually contributes to the final image.\n\nConsider a simple example: a line segment that extends far beyond the edges of your screen. Without clipping, the rasteriser (the process that converts 2D primitives into pixels) would attempt to draw pixels for the entire length of the line, most of which would be off-screen. Clipping neatly trims this line segment to the visible portion, saving computation and ensuring a clean image.","type":"content","url":"/lecture-6-gemini#why-bother-the-necessity-of-clipping","position":7},{"hierarchy":{"lvl1":"Chapter 6: Snipping Away the Unseen - The Art of Clipping","lvl2":"The Clipping Arena: Normalised Device Coordinates (NDC)"},"type":"lvl2","url":"/lecture-6-gemini#the-clipping-arena-normalised-device-coordinates-ndc","position":8},{"hierarchy":{"lvl1":"Chapter 6: Snipping Away the Unseen - The Art of Clipping","lvl2":"The Clipping Arena: Normalised Device Coordinates (NDC)"},"content":"Before we delve into specific clipping algorithms, it‚Äôs important to understand where this process typically takes place in the graphics pipeline. As we learned in earlier chapters, after the modelling transformations (positioning objects in the world) and the viewing transformation (positioning the camera and orienting the scene relative to it), the 3D scene is projected into Normalised Device Coordinates (NDC).\n\nIn NDC space, the viewing volume is typically a cube ranging from -1 to +1 in all three dimensions (x, y, and z). The near and far clipping planes of the view frustum are mapped to the z = -1 and z = +1 planes in NDC, respectively, and the image boundaries correspond to the x and y extents of this cube. Clipping operations are most conveniently performed in this NDC space, as the clip boundaries are well-defined and consistent.","type":"content","url":"/lecture-6-gemini#the-clipping-arena-normalised-device-coordinates-ndc","position":9},{"hierarchy":{"lvl1":"Chapter 6: Snipping Away the Unseen - The Art of Clipping","lvl2":"Families of Clipping Algorithms"},"type":"lvl2","url":"/lecture-6-gemini#families-of-clipping-algorithms","position":10},{"hierarchy":{"lvl1":"Chapter 6: Snipping Away the Unseen - The Art of Clipping","lvl2":"Families of Clipping Algorithms"},"content":"The sources introduce several algorithms tailored for different types of geometric primitives:\n\nPoint Clipping: The simplest form of clipping, determining if a point lies within the defined 2D clip window.\n\nLine Segment Clipping: Determining which portion (if any) of a line segment is visible within the clip window. Algorithms like Cohen-Sutherland and Liang-Barsky are discussed.\n\nPolygon Clipping: Clipping the edges and potentially creating new vertices for polygons that intersect the clip window boundaries. The Sutherland-Hodgeman algorithm is mentioned.\n\nLet‚Äôs explore the intuition and mathematics behind some of these.","type":"content","url":"/lecture-6-gemini#families-of-clipping-algorithms","position":11},{"hierarchy":{"lvl1":"Chapter 6: Snipping Away the Unseen - The Art of Clipping","lvl2":"Mathematical Intuition and Visual Examples"},"type":"lvl2","url":"/lecture-6-gemini#mathematical-intuition-and-visual-examples","position":12},{"hierarchy":{"lvl1":"Chapter 6: Snipping Away the Unseen - The Art of Clipping","lvl2":"Mathematical Intuition and Visual Examples"},"content":"","type":"content","url":"/lecture-6-gemini#mathematical-intuition-and-visual-examples","position":13},{"hierarchy":{"lvl1":"Chapter 6: Snipping Away the Unseen - The Art of Clipping","lvl3":"Point Clipping: A Simple Test","lvl2":"Mathematical Intuition and Visual Examples"},"type":"lvl3","url":"/lecture-6-gemini#point-clipping-a-simple-test","position":14},{"hierarchy":{"lvl1":"Chapter 6: Snipping Away the Unseen - The Art of Clipping","lvl3":"Point Clipping: A Simple Test","lvl2":"Mathematical Intuition and Visual Examples"},"content":"For a point p(x, y) to be inside a rectangular clip window defined by xmin, xmax, ymin, and ymax, it must satisfy the following simple inequalities:xmin ‚â§ x ‚â§ xmax\nymin ‚â§ y ‚â§ ymax\n\nVisual Example:\n\nImagine a rectangle representing our clip window. A point located anywhere within or on the boundary of this rectangle is considered visible. Any point outside these boundaries is clipped and not drawn.","type":"content","url":"/lecture-6-gemini#point-clipping-a-simple-test","position":15},{"hierarchy":{"lvl1":"Chapter 6: Snipping Away the Unseen - The Art of Clipping","lvl3":"Line Segment Clipping: Cohen-Sutherland - Divide and Conquer","lvl2":"Mathematical Intuition and Visual Examples"},"type":"lvl3","url":"/lecture-6-gemini#line-segment-clipping-cohen-sutherland-divide-and-conquer","position":16},{"hierarchy":{"lvl1":"Chapter 6: Snipping Away the Unseen - The Art of Clipping","lvl3":"Line Segment Clipping: Cohen-Sutherland - Divide and Conquer","lvl2":"Mathematical Intuition and Visual Examples"},"content":"The Cohen-Sutherland algorithm is an efficient approach for line segment clipping. It works by assigning a 4-bit outcode to each endpoint of the line segment. Each bit of the outcode corresponds to one of the four boundaries of the 2D clip window:\n\nBit 1 (Left): Set to 1 if the point is to the left of xmin.\n\nBit 2 (Right): Set to 1 if the point is to the right of xmax.\n\nBit 3 (Bottom): Set to 1 if the point is below ymin.\n\nBit 4 (Top): Set to 1 if the point is above ymax.\n\nIntuition: By examining the outcodes of the two endpoints, we can quickly determine if the entire line segment is trivially accepted (both outcodes are 0000, meaning both endpoints are inside) or trivially rejected (the bitwise AND of the two outcodes is not 0000, meaning both endpoints lie on the same side of at least one of the clipping boundaries).\n\nVisual Example:        Top (bit 4)\n          1001 | 1000 | 1010\n         ------|------|------\nLeft (bit 1) 0001 | 0000 | 0010 Right (bit 2)\n         ------|------|------\n          0101 | 0100 | 0110\n        Bottom (bit 3)\n\nIf a line segment has endpoints with outcodes 0000 and 0000 (e.g., P5 and P3 in), it‚Äôs entirely inside and trivially accepted. If both endpoints have their ‚ÄúLeft‚Äù bit set (e.g., a line above the ‚ÄúTop‚Äù boundary), their bitwise AND will also have the ‚ÄúTop‚Äù bit set, indicating trivial rejection.\n\nIf the line segment is neither trivially accepted nor rejected (the bitwise AND of the outcodes is 0000, but at least one outcode is not 0000), it means the line crosses one or more clipping boundaries. In this case, we need to find the intersection point(s) with the window edges. This can be done using the parametric equation of the line segment:x(Œ±) = x1 + Œ±(x2 - x1)\ny(Œ±) = y1 + Œ±(y2 - y1)\n\nwhere Œ± ranges from 0 to 1 for the segment between (x1, y1) and (x2, y2). We can substitute the clip window boundaries (e.g., x = xmin) into this equation to solve for the parameter Œ± at the intersection point. If 0 ‚â§ Œ± ‚â§ 1, the intersection point lies on the line segment. We then use this intersection point to replace the endpoint that lies outside the boundary and repeat the process until the entire segment is inside or trivially rejected.","type":"content","url":"/lecture-6-gemini#line-segment-clipping-cohen-sutherland-divide-and-conquer","position":17},{"hierarchy":{"lvl1":"Chapter 6: Snipping Away the Unseen - The Art of Clipping","lvl3":"Line Segment Clipping: Liang-Barsky - A Parametric Approach","lvl2":"Mathematical Intuition and Visual Examples"},"type":"lvl3","url":"/lecture-6-gemini#line-segment-clipping-liang-barsky-a-parametric-approach","position":18},{"hierarchy":{"lvl1":"Chapter 6: Snipping Away the Unseen - The Art of Clipping","lvl3":"Line Segment Clipping: Liang-Barsky - A Parametric Approach","lvl2":"Mathematical Intuition and Visual Examples"},"content":"The Liang-Barsky algorithm also leverages the parametric form of the line segment. It treats the clipping problem as finding the range of the parameter Œ± for which the line segment lies within the clip window. For each of the four clip boundaries, we can derive constraints on Œ±. For example, for the left boundary x = xmin:\n\nIf x2 - x1 < 0 (line goes from right to left), then Œ± > (xmin - x1) / (x2 - x1).\nIf x2 - x1 > 0 (line goes from left to right), then Œ± < (xmin - x1) / (x2 - x1).\n\nSimilar inequalities can be derived for the right, bottom, and top boundaries. By finding the intersection of these parameter ranges, we can determine the portion of the line segment that is inside the clip window. If the resulting valid range for Œ± is empty, the line segment is entirely outside.\n\nCore Idea\n\nThe Liang-Barsky algorithm is an efficient line clipping algorithm that leverages the parametric representation of a line segment.  Instead of directly computing intersection points, it determines the visible portion of the line by finding the range of the parameter alpha (Œ±) that corresponds to the part of the line inside the clipping window.  The algorithm works with rectangular clipping windows, defined by xmin, xmax, ymin, and ymax.\n\nParametric Equation of a Line Segment\n\nAny point (x, y) on a line segment between points (x1, y1) and (x2, y2) can be represented parametrically as:\n\nx = x1 + Œ±(x2 - x1)\n\ny = y1 + Œ±(y2 - y1)\n\nwhere 0 ‚â§ Œ± ‚â§ 1.\n\nŒ± = 0 corresponds to the point (x1, y1).\n\nŒ± = 1 corresponds to the point (x2, y2).\n\nValues of Œ± between 0 and 1 represent points along the line segment between the endpoints.\n\nValues of Œ± outside the range [0, 1] represent points on the infinite line extending beyond the line segment.\n\nThe Clipping Problem as a Range of Œ±\n\nThe Liang-Barsky algorithm‚Äôs key insight is that the clipping problem can be transformed into finding the valid range of Œ± for which the line segment lies within the clipping window.  That is, we want to find the values of Œ± such that:\n\nxmin ‚â§ x ‚â§ xmax\n\nymin ‚â§ y ‚â§ ymax\n\nSubstituting the parametric equations of the line segment into these inequalities gives us a way to constrain Œ±.\n\nDeriving the Constraints on Œ±\n\nLet‚Äôs consider each clipping boundary (left, right, bottom, top) separately:\n\nLeft Boundary (x = xmin):\n\nWe need xmin ‚â§ x = x1 + Œ±(x2 - x1)\n\nRearranging, we get xmin - x1 ‚â§ Œ±(x2 - x1)\n\nNow, we have two cases:\n\nCase 1: (x2 - x1) < 0 (Line goes from right to left):  Dividing by (x2 - x1) reverses the inequality sign:\nŒ± ‚â• (xmin - x1) / (x2 - x1)\n\nCase 2: (x2 - x1) > 0 (Line goes from left to right):  Dividing by (x2 - x1) preserves the inequality sign:\nŒ± ‚â§ (xmin - x1) / (x2 - x1)\n\n**Case 3: (x2-x1) = 0: ** The line is vertical.\n\nIf x1 < xmin, the line is totally on the left of the boundary and outside the window.\n\nIf x1 > xmin, the line is totally on the right of the boundary and can be inside the window.\n\nRight Boundary (x = xmax):\n\nWe need x = x1 + Œ±(x2 - x1) ‚â§ xmax\n\nRearranging, we get Œ±(x2 - x1) ‚â§ xmax - x1\n\nAgain, we have two cases:\n\nCase 1: (x2 - x1) < 0 (Line goes from right to left):\nŒ± ‚â§ (xmax - x1) / (x2 - x1)\n\nCase 2: (x2 - x1) > 0 (Line goes from left to right):\nŒ± ‚â• (xmax - x1) / (x2 - x1)\n\n**Case 3: (x2-x1) = 0: ** The line is vertical.\n\nIf x1 > xmax, the line is totally on the right of the boundary and outside the window.\n\nIf x1 < xmax, the line is totally on the left of the boundary and can be inside the window.\n\nBottom Boundary (y = ymin):\n\nWe need ymin ‚â§ y = y1 + Œ±(y2 - y1)\n\nRearranging, we get ymin - y1 ‚â§ Œ±(y2 - y1)\n\nCases:\n\n(y2 - y1) < 0: Œ± ‚â• (ymin - y1) / (y2 - y1)\n\n(y2 - y1) > 0: Œ± ‚â§ (ymin - y1) / (y2 - y1)\n\n**(y2-y1) = 0: ** The line is horizontal.\n\nIf y1 < ymin, the line is totally below the boundary and outside the window.\n\nIf y1 > ymin, the line is totally above the boundary and can be inside the window.\n\nTop Boundary (y = ymax):\n\nWe need y = y1 + Œ±(y2 - y1) ‚â§ ymax\n\nRearranging, we get Œ±(y2 - y1) ‚â§ ymax - y1\n\nCases:\n\n(y2 - y1) < 0: Œ± ‚â§ (ymax - y1) / (y2 - y1)\n\n(y2 - y1) > 0: Œ± ‚â• (ymax - y1) / (y2 - y1)\n\n**(y2-y1) = 0: ** The line is horizontal.\n\nIf y1 > ymax, the line is totally above the boundary and outside the window.\n\nIf y1 < ymax, the line is totally below the boundary and can be inside the window.\n\nDefining p and q\n\nTo summarize the inequalities, it‚Äôs common to introduce the following notation:\n\np1 = -(x2 - x1)\n\np2 = (x2 - x1)\n\np3 = -(y2 - y1)\n\np4 = (y2 - y1)\n\nq1 = (x1 - xmin)\n\nq2 = (xmax - x1)\n\nq3 = (y1 - ymin)\n\nq4 = (ymax - y1)\n\nUsing these p and q values, the constraints become:\n\nFor pk < 0: Œ± ‚â• qk / pk\n\nFor pk > 0: Œ± ‚â§ qk / pk\n\nFor pk = 0: The line is parallel to that boundary.  If qk < 0, the line is entirely outside the window and can be discarded.  If qk >= 0, the line might be inside, so consider the other boundaries.\n\nAlgorithm Steps\n\nCalculate p and q values:  Compute p1, p2, p3, p4 and q1, q2, q3, q4 as defined above.\n\nInitialize Œ±min and Œ±max:\n\nŒ±min = 0  (The initial minimum value of alpha, corresponding to the start point of the segment)\n\nŒ±max = 1  (The initial maximum value of alpha, corresponding to the end point of the segment)\n\nIterate through the boundaries (k = 1 to 4):\n\nIf pk = 0:  The line is parallel to this boundary.\n\nIf qk < 0, the line is entirely outside the clipping window.  Reject the line and stop.\n\nIf qk >= 0, continue to the next boundary.  This boundary does not affect the clipping.\n\nIf pk < 0:\n\nŒ± = qk / pk\n\nIf Œ± > Œ±max, the line is entirely outside the clipping window. Reject the line and stop.\n\nIf Œ± > Œ±min, then update Œ±min = Œ± (we‚Äôve found a larger minimum alpha value that clips the line).\n\nIf pk > 0:\n\nŒ± = qk / pk\n\nIf Œ± < Œ±min, the line is entirely outside the clipping window. Reject the line and stop.\n\nIf Œ± < Œ±max, then update Œ±max = Œ± (we‚Äôve found a smaller maximum alpha value that clips the line).\n\nCheck if Œ±min > Œ±max: If this is true, the line is entirely outside the clip window. Reject the line.\n\nDetermine the Clipped Line Segment:\n\nIf the algorithm hasn‚Äôt been stopped (i.e., the line is at least partially visible), calculate the new endpoints of the clipped line segment:\n\nx1' = x1 + Œ±min * (x2 - x1)\n\ny1' = y1 + Œ±min * (y2 - y1)\n\nx2' = x1 + Œ±max * (x2 - x1)\n\ny2' = y1 + Œ±max * (y2 - y1)\n\nThe line segment with endpoints (x1', y1') and (x2', y2') is the visible portion of the original line segment within the clipping window.\n\nExample\n\nLet‚Äôs say we have a line segment with endpoints (x1, y1) = (50, 50) and (x2, y2) = (150, 150), and the clipping window is defined by xmin = 75, xmax = 125, ymin = 75, ymax = 125.\n\nCalculate p and q:\n\np1 = -(150 - 50) = -100\n\np2 = (150 - 50) = 100\n\np3 = -(150 - 50) = -100\n\np4 = (150 - 50) = 100\n\nq1 = (50 - 75) = -25\n\nq2 = (125 - 50) = 75\n\nq3 = (50 - 75) = -25\n\nq4 = (125 - 50) = 75\n\nInitialize: Œ±min = 0, Œ±max = 1\n\nIterate through boundaries:\n\nk = 1 (Left): p1 = -100 < 0, Œ± = q1 / p1 = -25 / -100 = 0.25\n\nŒ± > Œ±min (0.25 > 0), so Œ±min = 0.25\n\nk = 2 (Right): p2 = 100 > 0, Œ± = q2 / p2 = 75 / 100 = 0.75\n\nŒ± < Œ±max (0.75 < 1), so Œ±max = 0.75\n\nk = 3 (Bottom): p3 = -100 < 0, Œ± = q3 / p3 = -25 / -100 = 0.25\n\nŒ± > Œ±min (0.25 > 0.25 is false, but the values are equal. This doesn‚Äôt change the result.  Some implementations will have the logic set up to include the equality case, and that is perfectly valid).\n\nk = 4 (Top): p4 = 100 > 0, Œ± = q4 / p4 = 75 / 100 = 0.75\n\nŒ± < Œ±max (0.75 < 0.75 is false, but the values are equal. This doesn‚Äôt change the result.  Some implementations will have the logic set up to include the equality case, and that is perfectly valid).\n\nCheck: Œ±min = 0.25, Œ±max = 0.75.  Œ±min <= Œ±max.\n\nCalculate Clipped Endpoints:\n\nx1' = 50 + 0.25 * (150 - 50) = 50 + 25 = 75\n\ny1' = 50 + 0.25 * (150 - 50) = 50 + 25 = 75\n\nx2' = 50 + 0.75 * (150 - 50) = 50 + 75 = 125\n\ny2' = 50 + 0.75 * (150 - 50) = 50 + 75 = 125\n\nThe clipped line segment has endpoints (75, 75) and (125, 125).\n\nAdvantages of Liang-Barsky over Cohen-Sutherland\n\nEfficiency: Liang-Barsky can be more efficient than Cohen-Sutherland, especially when a large portion of the line is inside the clip window. Cohen-Sutherland performs multiple clip tests and intersection calculations that can be avoided with the Liang-Barsky approach. Liang-Barsky calculates the p and q values once and reuses them for each boundary.  It also performs relatively few divisions.\n\nParametric Form: The parametric form allows for a more straightforward calculation of the clipped endpoints.\n\nRejection:  Liang-Barsky often rejects lines more quickly because it can determine the entire line is outside the window without calculating the intersections.\n\nIn summary, the Liang-Barsky algorithm is an efficient and elegant approach to line clipping that leverages the parametric representation of lines to minimize the number of calculations required.  It‚Äôs a valuable tool in computer graphics.","type":"content","url":"/lecture-6-gemini#line-segment-clipping-liang-barsky-a-parametric-approach","position":19},{"hierarchy":{"lvl1":"Chapter 6: Snipping Away the Unseen - The Art of Clipping","lvl3":"Polygon Clipping: Sutherland-Hodgeman - Clip Against Each Edge","lvl2":"Mathematical Intuition and Visual Examples"},"type":"lvl3","url":"/lecture-6-gemini#polygon-clipping-sutherland-hodgeman-clip-against-each-edge","position":20},{"hierarchy":{"lvl1":"Chapter 6: Snipping Away the Unseen - The Art of Clipping","lvl3":"Polygon Clipping: Sutherland-Hodgeman - Clip Against Each Edge","lvl2":"Mathematical Intuition and Visual Examples"},"content":"The Sutherland-Hodgeman algorithm clips a polygon against each edge of the clip window sequentially. It processes the polygon‚Äôs vertices one by one and determines whether to keep the vertex and/or introduce new vertices at the intersections with the clipping edge.\n\nFor each clipping edge, the algorithm considers four cases for consecutive vertices of the polygon:\n\nBoth vertices inside: Keep the second vertex.\n\nFirst vertex inside, second outside: Keep the intersection point.\n\nFirst vertex outside, second inside: Keep the intersection point and the second vertex.\n\nBoth vertices outside: Keep nothing.\n\nAfter processing all vertices against one clipping edge, the resulting list of vertices forms a new, partially clipped polygon. This process is repeated for the remaining three edges of the clip window.\n\nVisual Example:\n\nImagine a triangle being clipped by the left edge of the clip window. If the first vertex is inside and the second is outside, we calculate the intersection point and add it to our new polygon. If the next vertex is also outside, we add nothing. If the following vertex is inside, we calculate the intersection and add it, followed by the inside vertex itself. After processing all edges (left, right, bottom, top), we obtain the final clipped polygon.","type":"content","url":"/lecture-6-gemini#polygon-clipping-sutherland-hodgeman-clip-against-each-edge","position":21},{"hierarchy":{"lvl1":"Chapter 6: Snipping Away the Unseen - The Art of Clipping","lvl2":"Clipping in 3D"},"type":"lvl2","url":"/lecture-6-gemini#clipping-in-3d","position":22},{"hierarchy":{"lvl1":"Chapter 6: Snipping Away the Unseen - The Art of Clipping","lvl2":"Clipping in 3D"},"content":"The concepts of 2D clipping extend to 3D, where the clip volume is defined by the view frustum. Algorithms like Cohen-Sutherland can be adapted for 3D by using a 6-bit outcode (for the six clipping planes: left, right, top, bottom, near, and far). However, the fundamental principle of discarding geometry outside the viewing volume remains the same. Before projecting the 3D scene to 2D, clipping in 3D ensures that only the geometry within the camera‚Äôs view is processed further. The near and far clipping planes are crucial in 3D clipping, defining the depth range of visibility. Primitives with vertices both in front and behind the eye necessitate clipping against these planes.","type":"content","url":"/lecture-6-gemini#clipping-in-3d","position":23},{"hierarchy":{"lvl1":"Chapter 6: Snipping Away the Unseen - The Art of Clipping","lvl2":"The Importance Revisited"},"type":"lvl2","url":"/lecture-6-gemini#the-importance-revisited","position":24},{"hierarchy":{"lvl1":"Chapter 6: Snipping Away the Unseen - The Art of Clipping","lvl2":"The Importance Revisited"},"content":"Clipping is not merely a technical detail; it‚Äôs a cornerstone of efficient and visually correct rendering. By strategically removing unseen portions of our 3D world, we pave the way for faster frame rates and a more seamless experience for the viewer. It‚Äôs a testament to the power of careful geometric processing in creating the illusions we see on our screens.","type":"content","url":"/lecture-6-gemini#the-importance-revisited","position":25},{"hierarchy":{"lvl1":"Chapter 6: Snipping Away the Unseen - The Art of Clipping","lvl2":"Clipping in the Grand Scheme"},"type":"lvl2","url":"/lecture-6-gemini#clipping-in-the-grand-scheme","position":26},{"hierarchy":{"lvl1":"Chapter 6: Snipping Away the Unseen - The Art of Clipping","lvl2":"Clipping in the Grand Scheme"},"content":"As we‚Äôve seen, clipping sits squarely within the graphics pipeline. It comes after the transformation stages (modelling and viewing) that position and orient our scene, and before rasterisation turns our geometric primitives into pixels. The clipped primitives are then ready for the next crucial steps: determining visibility (which surfaces are in front of others) and finally, rendering the visible portions with the appropriate colours and shading. Understanding clipping is thus essential to grasping the entire flow of how a 3D scene is transformed into the 2D images we perceive.\n\nIn our continuing exploration, we will see how the clipped geometry is then processed to determine which pixels on the screen should be coloured and with what intensity. Clipping, in its silent efficiency, sets the stage for the visual magic that follows.","type":"content","url":"/lecture-6-gemini#clipping-in-the-grand-scheme","position":27},{"hierarchy":{"lvl1":"Chapter 7: Eliminating the Unseen and Painting the Surfaces"},"type":"lvl1","url":"/lecture-7-gemini","position":0},{"hierarchy":{"lvl1":"Chapter 7: Eliminating the Unseen and Painting the Surfaces"},"content":"","type":"content","url":"/lecture-7-gemini","position":1},{"hierarchy":{"lvl1":"Chapter 7: Eliminating the Unseen and Painting the Surfaces"},"type":"lvl1","url":"/lecture-7-gemini#chapter-7-eliminating-the-unseen-and-painting-the-surfaces","position":2},{"hierarchy":{"lvl1":"Chapter 7: Eliminating the Unseen and Painting the Surfaces"},"content":"This chapter delves into two critical stages of the graphics pipeline, often grouped together due to their close relationship in the final rendering process: √âlimination des Parties Cach√©es (Hidden Surface Removal) and Remplissage (Polygon Filling). Following the transformations, illumination, and clipping stages discussed in previous lectures, these steps are crucial for generating the final, viewable image from a 3D scene. Hidden surface removal addresses the fundamental problem of visibility ‚Äì determining which parts of the 3D objects are visible to the virtual camera and which are obscured by others. Once the visible surfaces are identified, polygon filling techniques come into play to render the interiors of these projected 2D polygons, providing a more realistic appearance to the objects.","type":"content","url":"/lecture-7-gemini#chapter-7-eliminating-the-unseen-and-painting-the-surfaces","position":3},{"hierarchy":{"lvl1":"Chapter 7: Eliminating the Unseen and Painting the Surfaces","lvl2":"The Challenge of Visibility: Hidden Surface Removal"},"type":"lvl2","url":"/lecture-7-gemini#the-challenge-of-visibility-hidden-surface-removal","position":4},{"hierarchy":{"lvl1":"Chapter 7: Eliminating the Unseen and Painting the Surfaces","lvl2":"The Challenge of Visibility: Hidden Surface Removal"},"content":"In a 3D scene, objects are positioned at varying depths relative to the camera. When these objects are projected onto a 2D image plane, some surfaces will inevitably be occluded by others that are closer to the viewpoint. The process of hidden surface removal aims to identify and discard these occluded portions, ensuring that only the visible parts of the scene are rendered. Several algorithms have been developed to tackle this problem, each with its own trade-offs in terms of computational cost, memory usage, and suitability for different types of scenes. These algorithms can be broadly categorised into object-space and image-space approaches.","type":"content","url":"/lecture-7-gemini#the-challenge-of-visibility-hidden-surface-removal","position":5},{"hierarchy":{"lvl1":"Chapter 7: Eliminating the Unseen and Painting the Surfaces","lvl3":"Object-Space Algorithms","lvl2":"The Challenge of Visibility: Hidden Surface Removal"},"type":"lvl3","url":"/lecture-7-gemini#object-space-algorithms","position":6},{"hierarchy":{"lvl1":"Chapter 7: Eliminating the Unseen and Painting the Surfaces","lvl3":"Object-Space Algorithms","lvl2":"The Challenge of Visibility: Hidden Surface Removal"},"content":"Object-space algorithms primarily work with the 3D geometry of the scene to determine visibility before projection or rasterisation.\n\nBackface Culling: This is a relatively simple yet effective preliminary step. It leverages the fact that for closed, solid objects, faces whose normals point away from the viewer are generally not visible. By calculating the dot product of the surface normal and the vector to the viewpoint, faces with a positive or zero dot product (depending on convention) can be discarded. This technique can save approximately 50% of the processing time on average and has a low cost per polygon. However, it is only sufficient for single, convex objects.\n\nPainter‚Äôs Algorithm: Also known as the ‚Äúdepth-sort algorithm‚Äù, this method aims to render polygons in order of decreasing depth from the viewer. The idea is analogous to how a painter layers paint, with closer objects painted last, obscuring those behind them. The algorithm first sorts all the polygons in the scene based on their farthest z-coordinate (depth). It then draws the polygons starting from the farthest to the nearest. While conceptually simple, the Painter‚Äôs Algorithm faces challenges with overlapping polygons where a simple depth sort is insufficient to determine the correct visibility order. In such ambiguous cases, polygon splitting might be necessary.\n\nBSP Trees (Binary Space Partition Trees): This approach involves recursively subdividing the 3D space containing the scene‚Äôs primitives using a series of splitting planes. The result is a hierarchical tree structure (a Directed Acyclic Graph - DAG) that efficiently organises the scene. To render the scene, the BSP tree is traversed recursively. For each node, the position of the viewpoint relative to the splitting plane is determined. The subtree on the far side of the plane (relative to the viewer) is rendered first, then the primitive associated with the current node (if any), and finally the subtree on the near side. This traversal order ensures that objects are drawn back-to-front, effectively resolving visibility in many cases. However, constructing the BSP tree can be computationally intensive, and splitting primitives can increase the complexity of the scene.","type":"content","url":"/lecture-7-gemini#object-space-algorithms","position":7},{"hierarchy":{"lvl1":"Chapter 7: Eliminating the Unseen and Painting the Surfaces","lvl3":"Image-Space Algorithms","lvl2":"The Challenge of Visibility: Hidden Surface Removal"},"type":"lvl3","url":"/lecture-7-gemini#image-space-algorithms","position":8},{"hierarchy":{"lvl1":"Chapter 7: Eliminating the Unseen and Painting the Surfaces","lvl3":"Image-Space Algorithms","lvl2":"The Challenge of Visibility: Hidden Surface Removal"},"content":"Image-space algorithms determine visibility after the scene has been projected onto the 2D image plane.\n\nWarnock Subdivision Algorithm: This algorithm employs a ‚Äúdivide and conquer‚Äù strategy by recursively subdividing the image plane into smaller quadrants (a quadtree). For each quadrant, it determines the list of potentially visible polygons. In simple cases, such as when a quadrant contains no polygons or only a single polygon that entirely covers it, the rendering is straightforward. However, when a quadrant contains multiple overlapping polygons, the algorithm attempts to determine if one polygon obscures all others within that region based on depth. If a clear frontmost polygon cannot be identified, the quadrant is further subdivided, and the process is repeated. The subdivision continues until each quadrant is smaller than a pixel, at which point the colour of the closest polygon is assigned.\n\nScan-Line Algorithms: These algorithms process the image row by row (scan line by scan line) to determine visibility. For each scan line, the algorithm identifies the intersections of the scan line with the edges of the projected polygons. These intersections divide the scan line into intervals. The visibility of the polygons within each interval is then determined by comparing their depths (z-values) at that particular scan line. Algorithms like Scan-Line-Watkins and Scan-Line-Z-buffer utilise this principle. The Scan-Line-Z-buffer method maintains a depth buffer (z-buffer) for the current scan line, storing the depth of the closest fragment encountered so far for each pixel along the line.\n\nZ-buffer Algorithm (Depth Buffer): The Z-buffer algorithm, introduced by Edwin Catmull, is one of the most widely used hidden surface removal techniques, particularly in hardware-accelerated graphics. It operates on a pixel-by-pixel basis and requires two buffers: a frame buffer to store the colour of each pixel and a z-buffer (depth buffer) to store the depth (z-value) of the object visible at each pixel. Initially, the z-buffer is filled with a value representing infinite depth, and the frame buffer is set to the background colour. As each polygon is rasterised (converted into fragments or pixels), the depth of the generated fragment is calculated. This depth is then compared to the value currently stored in the z-buffer at the corresponding pixel location. If the fragment‚Äôs depth is less than the stored depth (meaning it is closer to the viewer), the z-buffer is updated with the new depth, and the frame buffer is updated with the fragment‚Äôs colour. After processing all polygons, the frame buffer contains the final image with hidden surfaces correctly removed. The Z-buffer algorithm is relatively simple to implement, doesn‚Äôt require pre-sorting, and is highly parallelisable. However, it processes all polygons, even those that are entirely hidden, and has memory overhead for the z-buffer. It also doesn‚Äôt inherently handle transparency or inter-reflections correctly.","type":"content","url":"/lecture-7-gemini#image-space-algorithms","position":9},{"hierarchy":{"lvl1":"Chapter 7: Eliminating the Unseen and Painting the Surfaces","lvl3":"Choosing a Hidden Surface Removal Algorithm","lvl2":"The Challenge of Visibility: Hidden Surface Removal"},"type":"lvl3","url":"/lecture-7-gemini#choosing-a-hidden-surface-removal-algorithm","position":10},{"hierarchy":{"lvl1":"Chapter 7: Eliminating the Unseen and Painting the Surfaces","lvl3":"Choosing a Hidden Surface Removal Algorithm","lvl2":"The Challenge of Visibility: Hidden Surface Removal"},"content":"The choice of a particular hidden surface removal algorithm depends on various factors, including the complexity of the scene, the available hardware, and any specific rendering requirements beyond basic visibility. The Z-buffer algorithm is commonly provided by graphics libraries and hardware due to its simplicity and efficiency in many scenarios. Scan-line algorithms can be efficient when memory is limited or when specific rendering effects are desired that might be more easily integrated into the scan-line processing.","type":"content","url":"/lecture-7-gemini#choosing-a-hidden-surface-removal-algorithm","position":11},{"hierarchy":{"lvl1":"Chapter 7: Eliminating the Unseen and Painting the Surfaces","lvl2":"Filling the Polygons: Rendering the Interiors"},"type":"lvl2","url":"/lecture-7-gemini#filling-the-polygons-rendering-the-interiors","position":12},{"hierarchy":{"lvl1":"Chapter 7: Eliminating the Unseen and Painting the Surfaces","lvl2":"Filling the Polygons: Rendering the Interiors"},"content":"Once the visible polygons (or parts thereof after clipping) have been determined, the next step is to fill their interiors with the appropriate colours and attributes. This process, known as polygon filling or rasterisation, converts the 2D outline of the projected polygon into a set of discrete pixels on the screen and assigns them the calculated colours, often based on the illumination and shading models discussed previously.\n\nA common approach to polygon filling is the scan-line fill algorithm. For each scan line that intersects the polygon, the algorithm determines the segments of the scan line that lie within the polygon‚Äôs boundaries. This typically involves finding the intersection points of the scan line with the polygon edges, sorting these intersection points along the x-axis, and then filling the pixels between pairs of consecutive intersection points. A parity rule is often used to determine whether a point lies inside or outside the polygon. As the scan line moves across the polygon, the colour of the pixels being filled is determined by interpolating the colour or intensity values across the polygon, potentially using techniques like Gouraud or Phong shading.\n\nAnother category of filling algorithms includes seed fill or flood fill algorithms. These algorithms start from an initial pixel known to be inside the polygon (the ‚Äúseed‚Äù) and recursively or iteratively fill adjacent pixels that belong to the polygon‚Äôs interior based on a boundary condition (e.g., reaching an edge of a different colour or a predefined boundary).\n\nHandling boundary conflicts is an important consideration in polygon filling. Due to the discrete nature of pixels and potential approximations in edge tracing, intersection points might not fall exactly on pixel centres. Robust filling algorithms often implement rules to ensure that adjacent polygons sharing an edge do not leave gaps or overlap in the final rendered image.\n\nExpanding on:\nFilling the Polygons: Rasterisation\n\nThe terms ‚ÄúFilling the Polygons‚Äù and rasterisation essentially describe the process of converting the 2D projected primitives, which are often broken down into triangles, into a set of discrete pixels on the screen and assigning them colours. The goal is to render the interiors of these polygons, providing a more realistic and continuous appearance to the objects. The rasterisation pipeline takes 3D primitives as input and produces a bitmap image as output.\n\nWhy Triangles?\n\nThe sources highlight why triangles are the fundamental primitive for rasterisation:\n\nThey can approximate any shape.\n\nThey are always planar with a well-defined normal vector.\n\nIt is easy to interpolate data across a triangle.\n\nEven points and lines are conceptually converted into triangles within the rasterisation pipeline.\n\nThe Process of Rasterisation\n\nRasterisation involves determining which pixels on the screen are covered by the projected 2D primitive (typically a triangle). For each pixel covered, the rasteriser also interpolates values known at the vertices of the primitive, such as colour and depth, to determine the corresponding values for that pixel (which is often referred to as a fragment).\n\nPolygon Filling Algorithms\n\nThe sources discuss several methods for filling the projected polygons:\n\nTesting Point Membership: This involves determining if a given point lies inside a polygon. Different approaches exist for convex and concave polygons.\n\nConvex Polygons: One method involves determining ‚Äúexterior‚Äù normals and checking if the point lies on the correct side of all the edges (defined by the normals). Another test involves checking the sign of the cross product of consecutive edge vectors; for a convex polygon, the sign should remain consistent during traversal.\n\nConcave Polygons: Concave polygons can change their traversal direction.\n\nRay Casting (Number of Intersections Test): This involves drawing a ray from the point and counting the number of times it intersects the edges of the polygon. An odd number of intersections generally indicates the point is inside, while an even number indicates it‚Äôs outside. Care must be taken with edge cases like intersections at vertices or tangent edges.\n\nIntersection Tests: These algorithms determine which parts of the screen are covered by the polygon by finding intersections with scan lines or other geometric entities.\n\nScan-Line Algorithms: These algorithms process the image line by line (horizontally) within the bounding box of the polygon.\n\nFor each scan line, the algorithm finds the intersections between the scan line and the edges of the polygon. This can be done by approximating the segments using a line tracing algorithm.\n\nThe intersection points are collected and organised, often in a linked list, and sorted by their x-coordinates.\n\nA parity rule is then applied: as the scan line traverses the polygon, a parity counter is incremented each time an edge is crossed. Pixels are filled if the parity is odd, indicating they are inside the polygon.\n\nTo avoid gaps or overlaps along shared edges between polygons, algorithms need to handle boundary conflicts, often by considering points strictly inside the polygon.\n\nFor each involved side of the polygon and each processed scan line (yi), the calculation of the intersection requires the ymax, xmin, and the inverse of the slope (dx/dy) of the edge.\n\nRegion Filling (Seed Fill or Flood Fill): These algorithms operate on regions defined by a boundary. Starting from an interior ‚Äúseed‚Äù pixel, the filling colour is recursively or iteratively propagated to neighbouring pixels until the boundary of the region is reached.\n\nIntegration with Shading\n\nThe way polygons are filled is closely linked to the shading model being used. The shading model determines the colour of each pixel within the polygon. For example:\n\nFlat Shading (Lambert Shading): A single illumination value is calculated for the entire polygon, often at the midpoint, using the face normal.\n\nGouraud Shading: Illumination intensities are calculated at the vertices of the polygon and then bilinearly interpolated across the face. This helps to eliminate intensity discontinuities. The interpolation can be done edge by edge and then horizontally across the span. Gouraud shading is efficient and commonly implemented in graphics hardware.\n\nPhong Shading: Instead of interpolating intensities, Phong shading interpolates the surface normals at the vertices across the face. The illumination model is then applied at each pixel using the interpolated normal, resulting in more accurate specular highlights. This generally produces better results than Gouraud shading, especially for specular reflections, but is computationally more expensive.\n\nThe interpolation process during rasterisation often uses techniques like linear interpolation and barycentric coordinates. Barycentric coordinates can be used to interpolate any vertex attribute (colour, texture coordinates, etc.) across the triangle during rasterisation by measuring the ‚Äúdistance‚Äù to each edge or by using the ratio of triangle areas. For perspective projections, a perspective-correct interpolation method is needed.\n\nGraphics Pipeline Integration\n\nPolygon filling (rasterisation) is a crucial step in the graphics pipeline. The 2D projected and clipped primitives are fed into the rasteriser. The output is a set of pixel fragments with interpolated attributes like colour and depth, which are then passed on to the visibility determination stage (e.g., using a Z-buffer) and eventually written to the frame buffer.\n\nIn summary, ‚ÄúFilling the Polygons‚Äù or rasterisation is the process of converting 2D geometric primitives into pixels on the screen, using various algorithms to determine which pixels are inside the polygon and interpolating attributes across its surface, influenced by the chosen shading model, to create the final rendered image.","type":"content","url":"/lecture-7-gemini#filling-the-polygons-rendering-the-interiors","position":13},{"hierarchy":{"lvl1":"Chapter 7: Eliminating the Unseen and Painting the Surfaces","lvl2":"Integration with the Graphics Pipeline"},"type":"lvl2","url":"/lecture-7-gemini#integration-with-the-graphics-pipeline","position":14},{"hierarchy":{"lvl1":"Chapter 7: Eliminating the Unseen and Painting the Surfaces","lvl2":"Integration with the Graphics Pipeline"},"content":"Hidden surface removal and polygon filling are tightly integrated within the graphics pipeline. Following the projection and clipping stages, the 2D projected primitives are passed to the rasterisation stage, where polygon filling occurs. If a Z-buffer is employed, it works in conjunction with the rasterisation process. As each fragment of a polygon is generated during rasterisation, its depth is checked against the Z-buffer to determine visibility before its colour is written to the frame buffer. The output of these stages is the final 2D image, ready to be displayed.","type":"content","url":"/lecture-7-gemini#integration-with-the-graphics-pipeline","position":15},{"hierarchy":{"lvl1":"Chapter 7: Eliminating the Unseen and Painting the Surfaces","lvl2":"Conclusion"},"type":"lvl2","url":"/lecture-7-gemini#conclusion","position":16},{"hierarchy":{"lvl1":"Chapter 7: Eliminating the Unseen and Painting the Surfaces","lvl2":"Conclusion"},"content":"Lecture 7, covering hidden surface removal and polygon filling, addresses fundamental challenges in rendering realistic 3D graphics. By correctly determining which surfaces are visible and accurately filling the interiors of the projected polygons, these techniques are essential for creating the final image that represents the 3D scene from the camera‚Äôs perspective. The choice of algorithms for these stages significantly impacts the performance and visual quality of the rendered output, and a thorough understanding of these concepts is crucial for anyone working in the field of Informatique Graphique pour la Science des Donn√©es.","type":"content","url":"/lecture-7-gemini#conclusion","position":17},{"hierarchy":{"lvl1":"Methode du germe"},"type":"lvl1","url":"/methode-du-germe","position":0},{"hierarchy":{"lvl1":"Methode du germe"},"content":"The ‚ÄúRemplissage de r√©gions (m√©thode du germe)‚Äù is one of the methods listed for performing ‚ÄúRemplissage‚Äù. The general purpose of ‚ÄúRemplissage‚Äù is to fill projected 2D polygons to give objects a more realistic appearance, typically following a shading model like Gouraud, Phong, or Lambert.\n\nThe seed method specifically addresses regions defined by a boundary. Its core principle is to start from an interior pixel within the region to be filled (referred to as a ‚Äúgerme‚Äù or seed) and then recursively propagate the fill colour to the neighbours of this pixel until the boundary is reached.\n\nThe sources describe the process during each iteration:\n\nHorizontal Filling: For the current seed pixel(s), the algorithm fills all pixels horizontally to the right and to the left. This horizontal filling continues until a boundary colour is encountered.\n\nIdentifying New Seeds: Among the pixels located directly above and below the horizontal line segment(s) that were just filled, the algorithm searches for those. The specific pixels identified are those that are the leftmost of a maximal horizontal sequence eligible for filling. This means finding the start of a continuous horizontal run of pixels on the adjacent scanlines that are inside the region and haven‚Äôt been filled yet.\n\nStacking New Seeds: These newly identified ‚Äúleftmost‚Äù pixels (which represent the starting points for future filling operations) are then pushed onto a stack. These stacked pixels serve as the ‚Äúgermes‚Äù (seeds) for subsequent iterations, allowing the filling process to continue expanding into unexplored parts of the region.\n\nThis iterative process, using a stack to manage the seeds, ensures that the fill colour propagates throughout the entire connected region starting from the initial germe, stopping only when the defined boundary is met.\n\nIn the broader context of the graphics pipeline, ‚ÄúRemplissage‚Äù, including methods like the seed fill, happens after geometric transformations, projection onto the screen plane, and potentially after or alongside pixelisation and visibility/rendering stages.","type":"content","url":"/methode-du-germe","position":1},{"hierarchy":{"lvl1":""},"type":"lvl1","url":"/visual-explanations","position":0},{"hierarchy":{"lvl1":""},"content":"import numpy as np\nimport matplotlib.pyplot as plt","type":"content","url":"/visual-explanations","position":1}]}